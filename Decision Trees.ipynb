{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Trees\n",
    "\n",
    "The process of constructing a decision tree is loosely based on the concepts of **information entropy and information gain** from information theory. A decision tree is a graph that describes a model of decisions and their possible consequences. An internal node in a decision tree represents a decision, or rather a condition of a particular feature in the context of classification. It has two possible outcomes that are represented by the left and right subtrees of the node. Of course, a node in the decision tree coul also have more than two subtrees. Each leaf node represents a particular class.\n",
    "\n",
    "There are actually several algorithms that are used to construct a decision tree from some training data. Generally, the tree is constructed by splitting the set of sample values in the training data into smaller subsets based on an attribute value test. The process is repeated in each subset until splitting a given subset of sample values no longer adds internal nodes to the decision tree.\n",
    "\n",
    "Once a decision tree has been created, we can optionally perform **pruning** on the tree. Pruning is simply the process of removing any extraneous decision nodes from the tree. This can be thought as a form for the regularization of decision tree through which we prevent underfitting or overfitting of the estimated decision tree model.\n",
    "\n",
    "**J48** is an open source implementation of the **C4.5** algorithm in Java.\n",
    "\n",
    "**Pros** Computationally cheap to use, easy for humans to understand learned results, missing values OK, can deal with irrelevant features.\n",
    "**Cons**: Prone to overfitting\n",
    "**Works with**: Numeric values and  nominal values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function to calculate the Shannon Entropy of a dataset\n",
    "from math import log\n",
    "import operator\n",
    "\n",
    "def calcShannonEnt(dataset):\n",
    "    numEntries = len(dataset)\n",
    "    labelCounts = {}\n",
    "    for featVec in dataset:\n",
    "        currentLabel = featVec[-1] \n",
    "        #print currentLabel\n",
    "        if currentLabel not in labelCounts.keys():\n",
    "            labelCounts[currentLabel] = 1\n",
    "        else:\n",
    "            labelCounts[currentLabel] += 1\n",
    "            \n",
    "    #print labelCounts\n",
    "    shannonEnt = 0.0\n",
    "    for key in labelCounts:\n",
    "        prob = float(labelCounts[key]) / numEntries\n",
    "        #print prob\n",
    "        shannonEnt -= prob * log(prob,2)\n",
    "        \n",
    "    return shannonEnt\n",
    "\n",
    "def splitDataSet(dataset, axis, value):\n",
    "    retDataSet = []\n",
    "    for featVec in dataset:\n",
    "        if featVec[axis] == value:\n",
    "            reducedFeatVec = featVec[:axis]\n",
    "            reducedFeatVec.extend(featVec[axis+1:])\n",
    "            retDataSet.append(reducedFeatVec)\n",
    "    return retDataSet\n",
    "\n",
    "def createTree(dataset, labels):\n",
    "    classList = [example[-1] for example in dataset]\n",
    "    if classList.count(classList[0]) == len(classList):\n",
    "        return classList[0]\n",
    "    if len(dataset[0]) == 1:\n",
    "        return majorityCnt(classList)\n",
    "    bestFeature = chooseBestFeatureToSplit(dataset)\n",
    "    bestFeatureLabel = labels[bestFeature]\n",
    "    myTree = {bestFeatureLabel: {}}\n",
    "    del(labels[bestFeature])\n",
    "    featValues = [example[bestFeature] for example in dataset]\n",
    "    uniqueVals = set(featValues)\n",
    "    for value in uniqueVals:\n",
    "        subLabels = labels[:]\n",
    "        myTree[bestFeatureLabel][value] = createTree(splitDataSet\\\n",
    "                                                    (dataset, bestFeature, value), subLabels)\n",
    "    return myTree\n",
    "\n",
    "def majorityCnt(classList):\n",
    "    classCount = {}\n",
    "    for vote in classList:\n",
    "        if vote not in classCount.keys():\n",
    "            classCount[vote] = 1\n",
    "        else:\n",
    "            classList[vote] +=1\n",
    "    \n",
    "\n",
    "def chooseBestFeatureToSplit(dataset):\n",
    "    numFeatures = len(dataset[0]) - 1\n",
    "    baseEntropy = calcShannonEnt(dataset)\n",
    "    bestInfoGain = 0.0\n",
    "    bestFeature = -1\n",
    "    for i in range(numFeatures):\n",
    "        featList = [example[i] for example in dataset]\n",
    "        uniqueVals = set(featList)\n",
    "        newEntropy = 0.0\n",
    "        for value in uniqueVals:\n",
    "            subDataSet = splitDataSet(dataset, i, value)\n",
    "            prob = len(subDataSet)/float(len(dataset))\n",
    "            newEntropy += prob * calcShannonEnt(subDataSet)\n",
    "\n",
    "        infoGain = baseEntropy - newEntropy\n",
    "        if (infoGain > bestInfoGain):\n",
    "            bestInfoGain = infoGain\n",
    "            bestFeature = i\n",
    "\n",
    "    return bestFeature\n",
    "\n",
    "def createDataSet():\n",
    "    dataset = [[1, 1, 'yes'],\n",
    "              [1, 1, 'yes'],\n",
    "              [1, 0, 'no'],\n",
    "              [0, 1, 'no'],\n",
    "              [0, 1, 'no'],]\n",
    "\n",
    "    labels = ['no surfacing', 'flippers']\n",
    "\n",
    "    return dataset, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "myDat, labels = createDataSet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9709505944546686"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calcShannonEnt(myDat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.3709505944546687"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myDat[0][-1] = 'maybe'\n",
    "calcShannonEnt(myDat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 'maybe'], [1, 'yes'], [0, 'no']]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splitDataSet(myDat, 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chooseBestFeatureToSplit(myDat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'no surfacing': {0: 'no', 1: {'flippers': {0: 'no', 1: 'yes'}}}}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myDat, labels = createDataSet()\n",
    "myTree = createTree(myDat, labels)\n",
    "myTree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
