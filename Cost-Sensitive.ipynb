{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Besides tuning the thresholds of our classifier, there are other approaches you can take to aid with uneven clasification costs. There are many ways to include the cost information in classification algorithms. For example in naive Bayes, you could predict the class with the lowest expected cost instead of the class with the highest probability.\n",
    "\n",
    "Another way to tune classifieres is to alter the data used to train the classifier to deal with imbalanced classification tasks. This is done by either undersampling or oversampling data. **Oversample** means to duplicate examples, wheresas **undersample** means delete examples. The sampling can be done either randomply or in a predeterminated fashion. For example, one approach would be to keep all of the examples from the positive class and undersample or discard examples from the negative class. One drawback of this approach is deciding which negative examples to toss out. The examples you choose to toss out could carry valuable information that isn't contained in the remaining examples.\n",
    "\n",
    "To oversample the positive class, you could replicate the existing examples or add new points similar to the existing points. One approach is to add a data point **interpolated** between existing data points. This process can lead to **overfitting**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Exprected Value\n",
    "\n",
    "The expected value computation provides a framework that is extremely useful in organizing thinking about data-analytic problems. Specifically, it decomposes data-analytic thinking into:\n",
    "1. The structure of the problem\n",
    "2. The elements of the analysis that can be extracted from data\n",
    "3. The elements of the analysis that need to be acuired from other data sources\n",
    "\n",
    "The expected value is then the weighted average of the values of the different possible outcomes, where the weight given to each value it its probability of occurrence:\n",
    "$$EV=\\sum\\limits_{i=1}^n p(o_{i})* v(o_{i})$$\n",
    "\n",
    "The probabilities often can be estimated form data, but business values often need to be acquired from other sources.\n",
    "\n",
    "Instead of computing accuracies for the competing models, we would compute expected values. In many data mining studies, the focus is on minimizing cost rather than maximizing profit, so the signs are reversed. Mathematically, there is no difference. However, it is important to pick one view and be consistent. An easy mistake in formulating const-benefit matrices is to **\"double count\"** by putting a benefit in one cell and a negative cost for the same thing in another cell. **One should be zero**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
