{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Besides tuning the thresholds of our classifier, there are other approaches you can take to aid with uneven clasification costs. There are many ways to include the cost information in classification algorithms. For example in naive Bayes, you could predict the class with the lowest expected cost instead of the class with the highest probability.\n",
    "\n",
    "Another way to tune classifieres is to alter the data used to train the classifier to deal with imbalanced classification tasks. This is done by either undersampling or oversampling data. **Oversample** means to duplicate examples, wheresas **undersample** means delete examples. The sampling can be done either randomply or in a predeterminated fashion. For example, one approach would be to keep all of the examples from the positive class and undersample or discard examples from the negative class. One drawback of this approach is deciding which negative examples to toss out. The examples you choose to toss out could carry valuable information that isn't contained in the remaining examples.\n",
    "\n",
    "To oversample the positive class, you could replicate the existing examples or add new points similar to the existing points. One approach is to add a data point **interpolated** between existing data points. This process can lead to **overfitting**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
