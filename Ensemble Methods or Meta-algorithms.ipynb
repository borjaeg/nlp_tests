{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boosting\n",
    "It is a technique similar to bagging. In boosting and bagging, you always use the same type of classifier. But in boosting, the different classifiers are trained sequentally. Each new classifier is trained based on the performance of those already trained. Boosting makes new classifiers focus on data that was previously misclassified by previous classifiers.\n",
    "\n",
    "Boosting is different from bagging because the output is calculated from a **weighted sum of all classifiers**. The weights aren't equal as in bagging but are based on how successful the classifier was in the previous iteration. There are many versions of boosting, but the most popular is called **AdaBoost**.\n",
    "\n",
    "AdaBoost is considered by some to be the best-supervised learning algorithm.\n",
    "\n",
    "**Pros**\n",
    "Low generalization error, easy to code, works with most classifiers, no parameters to adjust\n",
    "\n",
    "**Cons**\n",
    "Sensitive to outliers\n",
    "\n",
    "**Works with**\n",
    "Numeric values, nominal values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bagging\n",
    "Bootstrap aggregating, which is known as bagging, is a technique where the data is trak from the original dataset S times to make S new datasets. The datasets are the same size as the original. Each dataset is built by randomly selecting an example from the original with replacement. It means, yo can select the same example more than once.\n",
    "\n",
    "After the S datasets are built, a learning algorithm is applied to each one individually. When you'd like to classify a new piece of data, you'd apply our S classifiers to the new piece of data and take a majority vote. There are more advanced methods of bagging such as **random forests**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forests\n",
    "\n",
    "RF are ensembles of decision trees. They do not require feature scaling, and are able to capture non-linearities and feature interactions. The algorithm injects randomness into the training process so that each tree is a bit different. Combining the predictions from each tree reduces the variance of predictions, improving the model performance on test data.\n",
    "\n",
    "**Predicion**\n",
    "    * Classification: Majoiry Vote\n",
    "    * Regression: Averaging\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Gradient-Boosted Trees vs. Random Forests\n",
    "\n",
    "Reference: http://spark.apache.org/docs/latest/mllib-ensembles.html\n",
    "\n",
    "Both are algorithms for learning ensembles of trees, but the training processes are different. There are several practical trade-offs:\n",
    "* GBTs train one tree at a time, so they can take longer to train than random forests. RF can train multiple trees in parallel. On the other hand, it is often reasonable to use smaller (shallower) trees with GBTs than with RF, and training smaller trees takes less time.\n",
    "* RF can be less prone to overfitting. Training more trees in RF reduces the likelihood of overfitting, but training more trees with GBTs increases the likelihood of overfitting. In statistical language, RF decrease the variance by using more trees, whereas GBTs reduce bias by using more trees.\n",
    "* Random Forests can be easier to tune since performance improves monotically with the number of trees whereas performance can start to decrease for GBTs if the number of trees grows too large."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
