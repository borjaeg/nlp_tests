{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularization\n",
    "\n",
    "**Overfitting** is a common problem in machine learning, where a model performs well on trainig data but does not generalize it order to predict unseen data. If a model suffers from overfitting, we can say that the model has a high variance, which can be casued, for examle, due to having too many parameters that lead to a model that is too complex. **Variance** measures the consistency (or variability) of the model prediction for a particular sample instance if we would retrain the model multiple times on different subsets of the training dataset. One way to finding a good bias-variance tradeoff is to tune the complexity of the model via **regularizaiton**. With this technique we can introduce additional information (bias) to penalize extreme parameter weights.\n",
    "\n",
    "Regularization is another reason why feature **scaling** such as standarizaiton is important. For regularization to work properly, it is necessary that all the features in the dataset are on the same scale. By increasing the regularization parameter, we increase the regularizaiton strength.\n",
    "\n",
    "The value of Î» increases the bias and lowers the variance ot the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
