{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Every application is different**. We cannot offer a single evaluation metric that is right for any classification problem, or regression problem, or whatever problem you may encounter. Ther term **machine learning diagnostic** is often used to describe a test that can be run to gain insight about what is and isn't working in a machine learning model. Generally, when designing a machine laerning model,it's advisable to formulate a diagnostic for the model in parallel.\n",
    "\n",
    "Another interesting aspect of machine learning is that without knowing the nature of the data we are trying to fit, we can make no assumption about which machine learning model we can use to fit the sample data. The axiom is known as the **No Free Lunch** theorem: \"Wothout prior assumptions about the nature of a learning algorithm, no learning algorithm is superior or inferior to any other (or even random guessing)\".\n",
    "\n",
    "Minimizing the error or cost function of a formulated machine learning model is generally not enough to determine how well the model fits the supplied training data.\n",
    "\n",
    "An estimated model is said to be **underfit** if it exhibits a large error in prediction. However, a formulated model with low error or cost function could also indicate that model doesn't understand the underlying relationship between the given features of the model. Rather, **the model is memorizing** the supplied data, and this known as **overfitting**. \n",
    "\n",
    "**An underfit model is also said to exhibit high bias and a overfit is said to have a high variance**. A model that describes a good fit for the sample data will have low overal error and can predict the dependent variable correctly from previously unseen values for the independent variables in our model.\n",
    "\n",
    "It is useful to think of a **positive example** as one worth of atterntion or **alarm**, and a negative example as uninteresting or benign.\n",
    "\n",
    "### Evaluating a Model\n",
    "\n",
    "We can plot the variance of the dependent and independent variables of a model to determine if the model is underfit or overfit.  However, with a parge number of feaures, we need a better way to visualize how well the model generalizes the relationship of the dependent and independet variables.\n",
    "\n",
    "If a selected model has a high error value for both training and cross-validation error, then the model is underfitting the supplied training data. On the other hand, a low training error and a high cross-validation error indicates that the model is overfit.\n",
    "\n",
    "### Learning curves\n",
    "\n",
    "A learning curve is essentially a plot of the error values in a model over the number of samples by which it is trained and cross-valdiated. Theu can be used to diagnose **underfit and overfit model**.\n",
    "\n",
    "### Plain Accuracy and its problems\n",
    "Classification accuracy is a popular metric because it's very easy to measure. Unfortunately, it is usually yoo simplistic for applications of data mining techniques to real business problems. To understand these problem we need a way to decompose and count different types of correct and incorrect decisions made by a classifier.\n",
    "\n",
    "$$\\text{accuracy} = \\cfrac{\\text{Number of correct decisions}}{\\text{Total number of decisions}}$$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix\n",
    "To evaluate a classifier proper√±y to understand the notion of **class confusion** is necessary to use the confusion matrix. It is a kind of contingency matrix. A confusion matrix separates the decision made by the classifier, making explicit how one class is being confused for another. In the confusion matrix, the main diagonal contains the counts of correct decisions. Confusion matrix is also known as **error matrix**.\n",
    "\n",
    "### Unbalanced classes\n",
    "Because the usunual or interesting class is rare among the general population, the class distribution is unbalanced or **skewed**. We need to know the proportion of positive or case of interest in our datasets. Unfortunately, as the class distribution becomes more skewed, evaluation based on accuracy breaks down. Even when the skew domains is not so great, in domains where one class is more prevalent than another accuracy can be greatly misleading. With such skewed domains the base rate for the majority class could be very high, so a report accuracy may tell us little about what data mining has really accomplished. Even when the skew it not so great, in domains where one class is more prevalent than another accuracy can be greatle misleading.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generalizing beyond classification\n",
    "Why is the mean-squared error on the predicted number of stars an appropriate metric for our recommendations problem? It it meaningful? is there a better metric? Hopefully, the analyst has thought this through carefully.\n",
    "\n",
    "While the probabilities can be estimated from data, the **costs** and **benefits** often cannot. They generally depend on external information provided by analysis of the consequences of decisions in the context of the specific **business problem**. Indeed, specifying the costs and benefits may take a great deal of time and though. In many cases, average estimated costs and benefits are used rather than individual-specific costs and benefits, for simplicity of problem formulation and calculation.\n",
    "\n",
    "Instead of computing accuracies for the competing models, we would compute expected values. Furthermore, using this alternative formulation, we can compare the two models even though one analyst tested using a representative distribution and the other tested using a class-balanced distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation, Baseline Performance, and Implications for Investments in Data\n",
    "It is important to consider carefully what would be a reasonable baseline against which to compare the model performance. A data scientitest will often need to implement an alternative model, usually one that is simple but not simplistic, in order to justify continuing the data mining effort. \n",
    "\n",
    "Each model performs considerably better than random guessing, and both are so easy to compute that they make natura baselines of comparison. Any new,more complex model must beat these. \n",
    "\n",
    "What are some general guidelines for good baselines? For classification task, one good baseline is the **majority classifier**, a naive classifier that always chooses that majority class of the training dataset.\n",
    "\n",
    "Maximizing simple prediction accuracy is usually not an appropriate goal. If that's what our algorithm is doing, we're using the wrong algorithm. For regression problem, we have a directly analougous baseline: predict the average value over the population.\n",
    "\n",
    "Moving beyond these sample baseline models, a slightly more complex alternative that only considers a very small amount of feature information. One xample of mining such single-feature predictive models from data is to use tree induction to build a **decision stump** - a decision tree with only one internal node, the root node.  A decision stump often produce quite good baseline performance on many of the test datasets used in machine learning reaearch.\n",
    "\n",
    "Beyond comparing simple models (and reduced-data models), it is often useful to implement simple, inexpensive models based on domain knowledge or \"receive wisdom\" and evaluate their performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Improving a model\n",
    "\n",
    "* Add or remove some features: this technique is used for both underfit and overfit model.\n",
    "* vary the value of the regularization parameter.\n",
    "* Gather more training data. For improving and overfit model.\n",
    "* Add features which are polynomial terms of the features in the model. This method can be used to improve an underfit model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"900\"\n",
       "            height=\"350\"\n",
       "            src=\"http://people.stern.nyu.edu/fprovost/Papers/rocch-mlj.pdf\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x103c0b9d0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# More Information\n",
    "from IPython.display import IFrame\n",
    "IFrame('http://people.stern.nyu.edu/fprovost/Papers/rocch-mlj.pdf', width=900, height=350)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"900\"\n",
       "            height=\"350\"\n",
       "            src=\"https://en.wikipedia.org/wiki/Precision_and_recall\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x103c0ba90>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# More Information\n",
    "from IPython.display import IFrame\n",
    "IFrame('https://en.wikipedia.org/wiki/Precision_and_recall', width=900, height=350)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"900\"\n",
       "            height=\"350\"\n",
       "            src=\"http://www.ucd.ie/geary/static/publications/workingpapers/gearywp201204.pdf\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x103c0ba10>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# More Information\n",
    "from IPython.display import IFrame\n",
    "IFrame('http://www.ucd.ie/geary/static/publications/workingpapers/gearywp201204.pdf', width=900, height=350)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
