{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk.stem\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "english_stemmer = nltk.stem.SnowballStemmer('english')\n",
    "\n",
    "class StemmedCountVectorizer(CountVectorizer):\n",
    "    def build_analyzer(self):\n",
    "        analyzer = super(CountVectorizer, self).build_analyzer()\n",
    "        return lambda doc: (english_stemmer.stem(w) for w in analyzer(doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vectorizer = StemmedCountVectorizer(min_df=1, stop_words='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "content = [\"How to format my hard disk\", \"Hard disk format problems\"]\n",
    "X = vectorizer.fit_transform(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'disk', u'format', u'hard', u'problem']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1 1 0]\n",
      " [1 1 1 1]]\n"
     ]
    }
   ],
   "source": [
    "print(X.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "class StemmedTfidfVectorizer(TfidfVectorizer):\n",
    "    def build_analyzer(self):\n",
    "        analyzer = super(TfidfVectorizer, self).build_analyzer()\n",
    "        \n",
    "        return lambda doc: (english_stemmer.stem(w) for w in analyzer(doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vectorizer = StemmedTfidfVectorizer(min_df = 1, stop_words='english', decode_error='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'disk', u'format', u'hard', u'problem']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.fit_transform(content)\n",
    "vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The bag of words model\n",
    "\n",
    "One of the most important sub-tasks in pattern classification are **feature extraction and selection**. Some important criteria are mentioned below:\n",
    "* Salient\n",
    "* Invariant\n",
    "* Discriminatory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming and Lemmatization\n",
    "\n",
    "**Stemming** is the process of transforming a word into its root form. But, this technique can create non-real words. In contras to stemming, **lemmatization** aims to obtain canonical (grammatically correct) forms of the words, the so-called **lemmas**. Lemmatization is computationaly more difficult and expensive than stemming, and in practice, both stemming and lemmatization have little impact on the performance of text classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N-grams\n",
    "\n",
    "In the n-gram model, a token can be defined as a sequence of n items. Choose the **optimal** number of n depends on the language as well as the particular application. Examples\n",
    "* Unigram: \"El\", \"perro\", \"come\"\n",
    "* Bigram: \"El perro\", \"perro come\"\n",
    "* Trigram: \"El perro come\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag words model drawbacks\n",
    "\n",
    "* It doesn't cover word relations\n",
    "* It doesn't capture negation correctly\n",
    "* It totally fails with misspelled words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Some experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_neighbors=2, p=2, weights='uniform')\n"
     ]
    }
   ],
   "source": [
    "from sklearn import neighbors\n",
    "\n",
    "knn = neighbors.KNeighborsClassifier(n_neighbors = 2)\n",
    "print knn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn.fit([[1], [2], [3], [4], [5], [6]], [0, 0, 0, 1, 1, 1])\n",
    "knn.predict(1.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn.predict(37)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn.predict(3.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.5,  0.5]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn.predict_proba(3.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.,  0.]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn.predict_proba(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "code_match = re.compile('<pre>(.*?)</pre>',\n",
    "                       re.MULTILINE | re.DOTALL)\n",
    "link_match = re.compile('<a href=\"http://.*?\".*?>(.*?)</a>',\n",
    "                       re.MULTILINE | re.DOTALL)\n",
    "tag_match = re.compile('<[^>]*>',\n",
    "                      re.MULTILINE | re.DOTALL)\n",
    "\n",
    "def extract_features_from_body(s):\n",
    "    link_count_in_code = 0\n",
    "    # count links in code\n",
    "    for match_str in code_match.findall(s):\n",
    "        link_count_in_code += len(link_match.findall(match_str))\n",
    "        \n",
    "    return len(link_match.findall(s)) - link_count_in_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = np.asarray([extract_features_from_body(text) for post_id, text in fetch_posts() \\\n",
    "               if post_id in all_answers])\n",
    "knn = neighbors.KNeighborsClassifier()\n",
    "knn.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = np.asarray([1,2,3,4,5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import KFold\n",
    "scores = []\n",
    "\n",
    "cv = KFold(n=len(X), n_folds=10, indices=True)\n",
    "\n",
    "for train, test in cv:\n",
    "    X_train, y_train = X[train], y[train]\n",
    "    X_test, y_test = X[test], y[test]\n",
    "    clf = neighbors.KNeighborsClassifier()\n",
    "    clf.fit(X,Y)\n",
    "    scores.append(clf.score(X_test, y_test))\n",
    "    \n",
    "print \"Mean(scores)=%.5f\\tStddev(scores)=%.5f\"\\\n",
    "        % (np.mean(scores), np.std(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Designing more features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_features_from_body(s):\n",
    "    num_code_lines = 0\n",
    "    link_count_in_code = 0\n",
    "    code_free_s = s\n",
    "    \n",
    "    for match_str in code_match.findall(s):\n",
    "        num_code_lines += match_str.count('\\n')\n",
    "        code_free_s = code_match.sub(\"\", code_free_s)\n",
    "        \n",
    "        link_count_in_code += len(link_match.findall(match_str))\n",
    "    \n",
    "    links = link_match.findall(s)\n",
    "    link_count = len(links)\n",
    "    link_count -= link_count_in_code\n",
    "    html_free_s = re.sub(\" +\", \" \", \n",
    "                        tag_match.sub(\"\", code_free_s)).replace(\"\\n\",\"\")\n",
    "    link_free_s = html_free_s\n",
    "    \n",
    "    for link in links:\n",
    "        if link.lower().startswith(\"http://\"):\n",
    "            link_free_s = link_free_s.replace(link, \"\")\n",
    "   \n",
    "    num_text_tokens = html_free_s.count(\" \")\n",
    "    \n",
    "    return num_text_tokens, num_code_lines, link_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## More examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "def create_ngram_model():\n",
    "    tfidf_ngrams = TfidfVectorizer(ngram_range = (1,3),\n",
    "                                  analyzer=\"word\", binary=False)\n",
    "    clf = MultinomialNB()\n",
    "    return Pipeline([('vect', tfidf_ngrams), ('clf', clf)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve, auc\n",
    "from sklearn.cross_validation import ShuffleSplit\n",
    "\n",
    "def train_model(clf_factory, X, Y):\n",
    "    cv = ShuffleSplit(n=len(X), n_iter = 10, test_size=0.3, random_state=0)\n",
    "    \n",
    "    scores = []\n",
    "    pr_scores = []\n",
    "    \n",
    "    for train, test in cv:\n",
    "        X_train, y_train = X[train], Y[train]\n",
    "        X_test, y_test = X[test], y[test]\n",
    "        \n",
    "        clf = clf_factory()\n",
    "        clf.fit(X_train, y_train)\n",
    "        \n",
    "        train_score = clf.score(X_train, y_train)\n",
    "        test_score = clf.score(X_test, y_test)\n",
    "        \n",
    "        scores.append(test_score)\n",
    "        proba = clf.predict_proba(X_test)\n",
    "        \n",
    "        precision, recall, pr_thresholds = \\\n",
    "        precision_recall_curve(y_test, proba[:,1])\n",
    "        \n",
    "        pr_scores.append(auc(recall, precision))\n",
    "        summary = (np.mean(scores), np.std(scores), np.mean(pr_scores), np.std(pr_scores))\n",
    "        print(\"%.3f\\t%.3f\\t%.3f\\t%.3f\" % summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def grid_search_model(clf_factory, X, Y):\n",
    "    cv = ShuffleSplit(\n",
    "        n = len(X), n_iter = 10, test_size = 0.3, random_state = 0\n",
    "    )\n",
    "    \n",
    "    param_grid = dict(vect__ngram_range=[(1,1),(1,2),(1,3)],\n",
    "                      vect__min_df=[1,2],\n",
    "                      vect__stop_words=[None, \"english\"],\n",
    "                      #vect__smooth_idf = [False, True],\n",
    "                      #vect__use_idf = [False, True],\n",
    "                      #vect__sublinear_tf = [False, True],\n",
    "                      #vect__binary = [False, True]\n",
    "                     )\n",
    "    \n",
    "    grid_search = GridSearchCV(clf_factory(),\n",
    "                              param_grid,\n",
    "                              cv = cv,\n",
    "                              score_func = f1_score,\n",
    "                              verbose=10)\n",
    "    grid_search.fit(X,Y)\n",
    "    \n",
    "    return grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 12 candidates, totalling 120 fits\n",
      "[CV] vect__ngram_range=(1, 1), vect__min_df=1, vect__stop_words=None .\n",
      "[CV]  vect__ngram_range=(1, 1), vect__min_df=1, vect__stop_words=None, score=0.914454 -   0.9s\n",
      "[CV] vect__ngram_range=(1, 1), vect__min_df=1, vect__stop_words=None .\n",
      "[CV]  vect__ngram_range=(1, 1), vect__min_df=1, vect__stop_words=None, score=0.896755 -   1.0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 jobs       | elapsed:    0.9s\n",
      "[Parallel(n_jobs=1)]: Done   2 jobs       | elapsed:    2.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[CV] vect__ngram_range=(1, 1), vect__min_df=1, vect__stop_words=None .\n",
      "[CV]  vect__ngram_range=(1, 1), vect__min_df=1, vect__stop_words=None, score=0.935103 -   1.1s\n",
      "[CV] vect__ngram_range=(1, 1), vect__min_df=1, vect__stop_words=None .\n",
      "[CV]  vect__ngram_range=(1, 1), vect__min_df=1, vect__stop_words=None, score=0.904130 -   0.9s\n",
      "[CV] vect__ngram_range=(1, 1), vect__min_df=1, vect__stop_words=None .\n",
      "[CV]  vect__ngram_range=(1, 1), vect__min_df=1, vect__stop_words=None, score=0.914454 -   1.1s\n",
      "[CV] vect__ngram_range=(1, 1), vect__min_df=1, vect__stop_words=None .\n",
      "[CV]  vect__ngram_range=(1, 1), vect__min_df=1, vect__stop_words=None, score=0.892330 -   1.0s\n",
      "[CV] vect__ngram_range=(1, 1), vect__min_df=1, vect__stop_words=None .\n",
      "[CV]  vect__ngram_range=(1, 1), vect__min_df=1, vect__stop_words=None, score=0.911504 -   1.0s\n",
      "[CV] vect__ngram_range=(1, 1), vect__min_df=1, vect__stop_words=None .\n",
      "[CV]  vect__ngram_range=(1, 1), vect__min_df=1, vect__stop_words=None, score=0.882006 -   1.3s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   5 jobs       | elapsed:    5.1s\n",
      "[Parallel(n_jobs=1)]: Done   8 jobs       | elapsed:    8.4s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[CV] vect__ngram_range=(1, 1), vect__min_df=1, vect__stop_words=None .\n",
      "[CV]  vect__ngram_range=(1, 1), vect__min_df=1, vect__stop_words=None, score=0.910029 -   1.6s\n",
      "[CV] vect__ngram_range=(1, 1), vect__min_df=1, vect__stop_words=None .\n",
      "[CV]  vect__ngram_range=(1, 1), vect__min_df=1, vect__stop_words=None, score=0.907080 -   1.7s\n",
      "[CV] vect__ngram_range=(1, 1), vect__min_df=1, vect__stop_words=english \n",
      "[CV]  vect__ngram_range=(1, 1), vect__min_df=1, vect__stop_words=english, score=0.952802 -   1.5s\n",
      "[CV] vect__ngram_range=(1, 1), vect__min_df=1, vect__stop_words=english \n",
      "[CV]  vect__ngram_range=(1, 1), vect__min_df=1, vect__stop_words=english, score=0.949853 -   1.2s\n",
      "[CV] vect__ngram_range=(1, 1), vect__min_df=1, vect__stop_words=english \n",
      "[CV]  vect__ngram_range=(1, 1), vect__min_df=1, vect__stop_words=english, score=0.974926 -   1.1s\n",
      "[CV] vect__ngram_range=(1, 1), vect__min_df=1, vect__stop_words=english \n",
      "[CV]  vect__ngram_range=(1, 1), vect__min_df=1, vect__stop_words=english, score=0.964602 -   1.0s\n",
      "[CV] vect__ngram_range=(1, 1), vect__min_df=1, vect__stop_words=english \n",
      "[CV]  vect__ngram_range=(1, 1), vect__min_df=1, vect__stop_words=english, score=0.948378 -   1.3s\n",
      "[CV] vect__ngram_range=(1, 1), vect__min_df=1, vect__stop_words=english \n",
      "[CV]  vect__ngram_range=(1, 1), vect__min_df=1, vect__stop_words=english, score=0.949853 -   1.0s\n",
      "[CV] vect__ngram_range=(1, 1), vect__min_df=1, vect__stop_words=english \n",
      "[CV]  vect__ngram_range=(1, 1), vect__min_df=1, vect__stop_words=english, score=0.960177 -   0.9s\n",
      "[CV] vect__ngram_range=(1, 1), vect__min_df=1, vect__stop_words=english \n",
      "[CV]  vect__ngram_range=(1, 1), vect__min_df=1, vect__stop_words=english, score=0.948378 -   1.3s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  13 jobs       | elapsed:   15.5s\n",
      "[Parallel(n_jobs=1)]: Done  18 jobs       | elapsed:   21.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[CV] vect__ngram_range=(1, 1), vect__min_df=1, vect__stop_words=english \n",
      "[CV]  vect__ngram_range=(1, 1), vect__min_df=1, vect__stop_words=english, score=0.955752 -   1.2s\n",
      "[CV] vect__ngram_range=(1, 1), vect__min_df=1, vect__stop_words=english \n",
      "[CV]  vect__ngram_range=(1, 1), vect__min_df=1, vect__stop_words=english, score=0.958702 -   1.4s\n",
      "[CV] vect__ngram_range=(1, 2), vect__min_df=1, vect__stop_words=None .\n",
      "[CV]  vect__ngram_range=(1, 2), vect__min_df=1, vect__stop_words=None, score=0.911504 -   5.2s\n",
      "[CV] vect__ngram_range=(1, 2), vect__min_df=1, vect__stop_words=None .\n",
      "[CV]  vect__ngram_range=(1, 2), vect__min_df=1, vect__stop_words=None, score=0.896755 -   4.3s\n",
      "[CV] vect__ngram_range=(1, 2), vect__min_df=1, vect__stop_words=None .\n",
      "[CV]  vect__ngram_range=(1, 2), vect__min_df=1, vect__stop_words=None, score=0.924779 -   4.2s\n",
      "[CV] vect__ngram_range=(1, 2), vect__min_df=1, vect__stop_words=None .\n",
      "[CV]  vect__ngram_range=(1, 2), vect__min_df=1, vect__stop_words=None, score=0.907080 -   4.8s\n",
      "[CV] vect__ngram_range=(1, 2), vect__min_df=1, vect__stop_words=None .\n",
      "[CV]  vect__ngram_range=(1, 2), vect__min_df=1, vect__stop_words=None, score=0.901180 -   4.6s\n",
      "[CV] vect__ngram_range=(1, 2), vect__min_df=1, vect__stop_words=None .\n",
      "[CV]  vect__ngram_range=(1, 2), vect__min_df=1, vect__stop_words=None, score=0.883481 -   4.3s\n",
      "[CV] vect__ngram_range=(1, 2), vect__min_df=1, vect__stop_words=None .\n",
      "[CV]  vect__ngram_range=(1, 2), vect__min_df=1, vect__stop_words=None, score=0.908555 -   5.4s\n",
      "[CV] vect__ngram_range=(1, 2), vect__min_df=1, vect__stop_words=None .\n",
      "[CV]  vect__ngram_range=(1, 2), vect__min_df=1, vect__stop_words=None, score=0.880531 -   4.6s\n",
      "[CV] vect__ngram_range=(1, 2), vect__min_df=1, vect__stop_words=None .\n",
      "[CV]  vect__ngram_range=(1, 2), vect__min_df=1, vect__stop_words=None, score=0.905605 -   5.0s\n",
      "[CV] vect__ngram_range=(1, 2), vect__min_df=1, vect__stop_words=None .\n",
      "[CV]  vect__ngram_range=(1, 2), vect__min_df=1, vect__stop_words=None, score=0.899705 -   5.2s\n",
      "[CV] vect__ngram_range=(1, 2), vect__min_df=1, vect__stop_words=english \n",
      "[CV]  vect__ngram_range=(1, 2), vect__min_df=1, vect__stop_words=english, score=0.955752 -   3.7s\n",
      "[CV] vect__ngram_range=(1, 2), vect__min_df=1, vect__stop_words=english \n",
      "[CV]  vect__ngram_range=(1, 2), vect__min_df=1, vect__stop_words=english, score=0.958702 -   3.4s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  25 jobs       | elapsed:   47.1s\n",
      "[Parallel(n_jobs=1)]: Done  32 jobs       | elapsed:  1.3min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[CV] vect__ngram_range=(1, 2), vect__min_df=1, vect__stop_words=english \n",
      "[CV]  vect__ngram_range=(1, 2), vect__min_df=1, vect__stop_words=english, score=0.979351 -   3.6s\n",
      "[CV] vect__ngram_range=(1, 2), vect__min_df=1, vect__stop_words=english \n",
      "[CV]  vect__ngram_range=(1, 2), vect__min_df=1, vect__stop_words=english, score=0.969027 -   3.7s\n",
      "[CV] vect__ngram_range=(1, 2), vect__min_df=1, vect__stop_words=english \n",
      "[CV]  vect__ngram_range=(1, 2), vect__min_df=1, vect__stop_words=english, score=0.954277 -   3.8s\n",
      "[CV] vect__ngram_range=(1, 2), vect__min_df=1, vect__stop_words=english \n",
      "[CV]  vect__ngram_range=(1, 2), vect__min_df=1, vect__stop_words=english, score=0.955752 -   3.8s\n",
      "[CV] vect__ngram_range=(1, 2), vect__min_df=1, vect__stop_words=english \n",
      "[CV]  vect__ngram_range=(1, 2), vect__min_df=1, vect__stop_words=english, score=0.966077 -   3.2s\n",
      "[CV] vect__ngram_range=(1, 2), vect__min_df=1, vect__stop_words=english \n",
      "[CV]  vect__ngram_range=(1, 2), vect__min_df=1, vect__stop_words=english, score=0.957227 -   3.0s\n",
      "[CV] vect__ngram_range=(1, 2), vect__min_df=1, vect__stop_words=english \n",
      "[CV]  vect__ngram_range=(1, 2), vect__min_df=1, vect__stop_words=english, score=0.955752 -   3.0s\n",
      "[CV] vect__ngram_range=(1, 2), vect__min_df=1, vect__stop_words=english \n",
      "[CV]  vect__ngram_range=(1, 2), vect__min_df=1, vect__stop_words=english, score=0.963127 -   3.2s\n",
      "[CV] vect__ngram_range=(1, 3), vect__min_df=1, vect__stop_words=None .\n",
      "[CV]  vect__ngram_range=(1, 3), vect__min_df=1, vect__stop_words=None, score=0.924779 -   8.2s\n",
      "[CV] vect__ngram_range=(1, 3), vect__min_df=1, vect__stop_words=None .\n",
      "[CV]  vect__ngram_range=(1, 3), vect__min_df=1, vect__stop_words=None, score=0.910029 -   8.6s\n",
      "[CV] vect__ngram_range=(1, 3), vect__min_df=1, vect__stop_words=None .\n",
      "[CV]  vect__ngram_range=(1, 3), vect__min_df=1, vect__stop_words=None, score=0.936578 -   8.6s\n",
      "[CV] vect__ngram_range=(1, 3), vect__min_df=1, vect__stop_words=None .\n",
      "[CV]  vect__ngram_range=(1, 3), vect__min_df=1, vect__stop_words=None, score=0.921829 -   8.7s\n",
      "[CV] vect__ngram_range=(1, 3), vect__min_df=1, vect__stop_words=None .\n",
      "[CV]  vect__ngram_range=(1, 3), vect__min_df=1, vect__stop_words=None, score=0.915929 -   9.1s\n",
      "[CV] vect__ngram_range=(1, 3), vect__min_df=1, vect__stop_words=None .\n",
      "[CV]  vect__ngram_range=(1, 3), vect__min_df=1, vect__stop_words=None, score=0.904130 -   8.5s\n",
      "[CV] vect__ngram_range=(1, 3), vect__min_df=1, vect__stop_words=None .\n",
      "[CV]  vect__ngram_range=(1, 3), vect__min_df=1, vect__stop_words=None, score=0.921829 -   8.6s\n",
      "[CV] vect__ngram_range=(1, 3), vect__min_df=1, vect__stop_words=None .\n",
      "[CV]  vect__ngram_range=(1, 3), vect__min_df=1, vect__stop_words=None, score=0.890855 -   8.4s\n",
      "[CV] vect__ngram_range=(1, 3), vect__min_df=1, vect__stop_words=None .\n",
      "[CV]  vect__ngram_range=(1, 3), vect__min_df=1, vect__stop_words=None, score=0.911504 -   8.4s\n",
      "[CV] vect__ngram_range=(1, 3), vect__min_df=1, vect__stop_words=None .\n",
      "[CV]  vect__ngram_range=(1, 3), vect__min_df=1, vect__stop_words=None, score=0.920354 -   8.4s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  41 jobs       | elapsed:  1.9min\n",
      "[Parallel(n_jobs=1)]: Done  50 jobs       | elapsed:  3.2min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[CV] vect__ngram_range=(1, 3), vect__min_df=1, vect__stop_words=english \n",
      "[CV]  vect__ngram_range=(1, 3), vect__min_df=1, vect__stop_words=english, score=0.954277 -   5.3s\n",
      "[CV] vect__ngram_range=(1, 3), vect__min_df=1, vect__stop_words=english \n",
      "[CV]  vect__ngram_range=(1, 3), vect__min_df=1, vect__stop_words=english, score=0.964602 -   5.5s\n",
      "[CV] vect__ngram_range=(1, 3), vect__min_df=1, vect__stop_words=english \n",
      "[CV]  vect__ngram_range=(1, 3), vect__min_df=1, vect__stop_words=english, score=0.979351 -   7.2s\n",
      "[CV] vect__ngram_range=(1, 3), vect__min_df=1, vect__stop_words=english \n",
      "[CV]  vect__ngram_range=(1, 3), vect__min_df=1, vect__stop_words=english, score=0.973451 -   5.9s\n",
      "[CV] vect__ngram_range=(1, 3), vect__min_df=1, vect__stop_words=english \n",
      "[CV]  vect__ngram_range=(1, 3), vect__min_df=1, vect__stop_words=english, score=0.961652 -   5.7s\n",
      "[CV] vect__ngram_range=(1, 3), vect__min_df=1, vect__stop_words=english \n",
      "[CV]  vect__ngram_range=(1, 3), vect__min_df=1, vect__stop_words=english, score=0.958702 -   5.6s\n",
      "[CV] vect__ngram_range=(1, 3), vect__min_df=1, vect__stop_words=english \n",
      "[CV]  vect__ngram_range=(1, 3), vect__min_df=1, vect__stop_words=english, score=0.969027 -   5.4s\n",
      "[CV] vect__ngram_range=(1, 3), vect__min_df=1, vect__stop_words=english \n",
      "[CV]  vect__ngram_range=(1, 3), vect__min_df=1, vect__stop_words=english, score=0.961652 -   6.8s\n",
      "[CV] vect__ngram_range=(1, 3), vect__min_df=1, vect__stop_words=english \n",
      "[CV]  vect__ngram_range=(1, 3), vect__min_df=1, vect__stop_words=english, score=0.960177 -   6.6s\n",
      "[CV] vect__ngram_range=(1, 3), vect__min_df=1, vect__stop_words=english \n",
      "[CV]  vect__ngram_range=(1, 3), vect__min_df=1, vect__stop_words=english, score=0.967552 -   5.5s\n",
      "[CV] vect__ngram_range=(1, 1), vect__min_df=2, vect__stop_words=None .\n",
      "[CV]  vect__ngram_range=(1, 1), vect__min_df=2, vect__stop_words=None, score=0.920354 -   1.0s\n",
      "[CV] vect__ngram_range=(1, 1), vect__min_df=2, vect__stop_words=None .\n",
      "[CV]  vect__ngram_range=(1, 1), vect__min_df=2, vect__stop_words=None, score=0.911504 -   1.0s\n",
      "[CV] vect__ngram_range=(1, 1), vect__min_df=2, vect__stop_words=None .\n",
      "[CV]  vect__ngram_range=(1, 1), vect__min_df=2, vect__stop_words=None, score=0.942478 -   1.1s\n",
      "[CV] vect__ngram_range=(1, 1), vect__min_df=2, vect__stop_words=None .\n",
      "[CV]  vect__ngram_range=(1, 1), vect__min_df=2, vect__stop_words=None, score=0.918879 -   0.9s\n",
      "[CV] vect__ngram_range=(1, 1), vect__min_df=2, vect__stop_words=None .\n",
      "[CV]  vect__ngram_range=(1, 1), vect__min_df=2, vect__stop_words=None, score=0.923304 -   1.1s\n",
      "[CV] vect__ngram_range=(1, 1), vect__min_df=2, vect__stop_words=None .\n",
      "[CV]  vect__ngram_range=(1, 1), vect__min_df=2, vect__stop_words=None, score=0.915929 -   1.0s\n",
      "[CV] vect__ngram_range=(1, 1), vect__min_df=2, vect__stop_words=None .\n",
      "[CV]  vect__ngram_range=(1, 1), vect__min_df=2, vect__stop_words=None, score=0.929204 -   1.0s\n",
      "[CV] vect__ngram_range=(1, 1), vect__min_df=2, vect__stop_words=None .\n",
      "[CV]  vect__ngram_range=(1, 1), vect__min_df=2, vect__stop_words=None, score=0.911504 -   1.1s\n",
      "[CV] vect__ngram_range=(1, 1), vect__min_df=2, vect__stop_words=None .\n",
      "[CV]  vect__ngram_range=(1, 1), vect__min_df=2, vect__stop_words=None, score=0.920354 -   1.1s\n",
      "[CV] vect__ngram_range=(1, 1), vect__min_df=2, vect__stop_words=None .\n",
      "[CV]  vect__ngram_range=(1, 1), vect__min_df=2, vect__stop_words=None, score=0.923304 -   1.0s\n",
      "[CV] vect__ngram_range=(1, 1), vect__min_df=2, vect__stop_words=english \n",
      "[CV]  vect__ngram_range=(1, 1), vect__min_df=2, vect__stop_words=english, score=0.958702 -   1.1s\n",
      "[CV] vect__ngram_range=(1, 1), vect__min_df=2, vect__stop_words=english \n",
      "[CV]  vect__ngram_range=(1, 1), vect__min_df=2, vect__stop_words=english, score=0.955752 -   1.0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  61 jobs       | elapsed:  4.3min\n",
      "[Parallel(n_jobs=1)]: Done  72 jobs       | elapsed:  4.5min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[CV] vect__ngram_range=(1, 1), vect__min_df=2, vect__stop_words=english \n",
      "[CV]  vect__ngram_range=(1, 1), vect__min_df=2, vect__stop_words=english, score=0.973451 -   1.0s\n",
      "[CV] vect__ngram_range=(1, 1), vect__min_df=2, vect__stop_words=english \n",
      "[CV]  vect__ngram_range=(1, 1), vect__min_df=2, vect__stop_words=english, score=0.971976 -   1.0s\n",
      "[CV] vect__ngram_range=(1, 1), vect__min_df=2, vect__stop_words=english \n",
      "[CV]  vect__ngram_range=(1, 1), vect__min_df=2, vect__stop_words=english, score=0.954277 -   1.0s\n",
      "[CV] vect__ngram_range=(1, 1), vect__min_df=2, vect__stop_words=english \n",
      "[CV]  vect__ngram_range=(1, 1), vect__min_df=2, vect__stop_words=english, score=0.958702 -   0.9s\n",
      "[CV] vect__ngram_range=(1, 1), vect__min_df=2, vect__stop_words=english \n",
      "[CV]  vect__ngram_range=(1, 1), vect__min_df=2, vect__stop_words=english, score=0.961652 -   1.1s\n",
      "[CV] vect__ngram_range=(1, 1), vect__min_df=2, vect__stop_words=english \n",
      "[CV]  vect__ngram_range=(1, 1), vect__min_df=2, vect__stop_words=english, score=0.955752 -   1.0s\n",
      "[CV] vect__ngram_range=(1, 1), vect__min_df=2, vect__stop_words=english \n",
      "[CV]  vect__ngram_range=(1, 1), vect__min_df=2, vect__stop_words=english, score=0.957227 -   0.9s\n",
      "[CV] vect__ngram_range=(1, 1), vect__min_df=2, vect__stop_words=english \n",
      "[CV]  vect__ngram_range=(1, 1), vect__min_df=2, vect__stop_words=english, score=0.958702 -   1.0s\n",
      "[CV] vect__ngram_range=(1, 2), vect__min_df=2, vect__stop_words=None .\n",
      "[CV]  vect__ngram_range=(1, 2), vect__min_df=2, vect__stop_words=None, score=0.923304 -   3.9s\n",
      "[CV] vect__ngram_range=(1, 2), vect__min_df=2, vect__stop_words=None .\n",
      "[CV]  vect__ngram_range=(1, 2), vect__min_df=2, vect__stop_words=None, score=0.911504 -   3.9s\n",
      "[CV] vect__ngram_range=(1, 2), vect__min_df=2, vect__stop_words=None .\n",
      "[CV]  vect__ngram_range=(1, 2), vect__min_df=2, vect__stop_words=None, score=0.939528 -   4.0s\n",
      "[CV] vect__ngram_range=(1, 2), vect__min_df=2, vect__stop_words=None .\n",
      "[CV]  vect__ngram_range=(1, 2), vect__min_df=2, vect__stop_words=None, score=0.924779 -   4.0s\n",
      "[CV] vect__ngram_range=(1, 2), vect__min_df=2, vect__stop_words=None .\n",
      "[CV]  vect__ngram_range=(1, 2), vect__min_df=2, vect__stop_words=None, score=0.923304 -   3.9s\n",
      "[CV] vect__ngram_range=(1, 2), vect__min_df=2, vect__stop_words=None .\n",
      "[CV]  vect__ngram_range=(1, 2), vect__min_df=2, vect__stop_words=None, score=0.917404 -   3.8s\n",
      "[CV] vect__ngram_range=(1, 2), vect__min_df=2, vect__stop_words=None .\n",
      "[CV]  vect__ngram_range=(1, 2), vect__min_df=2, vect__stop_words=None, score=0.927729 -   4.0s\n",
      "[CV] vect__ngram_range=(1, 2), vect__min_df=2, vect__stop_words=None .\n",
      "[CV]  vect__ngram_range=(1, 2), vect__min_df=2, vect__stop_words=None, score=0.910029 -   3.8s\n",
      "[CV] vect__ngram_range=(1, 2), vect__min_df=2, vect__stop_words=None .\n",
      "[CV]  vect__ngram_range=(1, 2), vect__min_df=2, vect__stop_words=None, score=0.915929 -   3.8s\n",
      "[CV] vect__ngram_range=(1, 2), vect__min_df=2, vect__stop_words=None .\n",
      "[CV]  vect__ngram_range=(1, 2), vect__min_df=2, vect__stop_words=None, score=0.921829 -   4.0s\n",
      "[CV] vect__ngram_range=(1, 2), vect__min_df=2, vect__stop_words=english \n",
      "[CV]  vect__ngram_range=(1, 2), vect__min_df=2, vect__stop_words=english, score=0.955752 -   4.0s\n",
      "[CV] vect__ngram_range=(1, 2), vect__min_df=2, vect__stop_words=english \n",
      "[CV]  vect__ngram_range=(1, 2), vect__min_df=2, vect__stop_words=english, score=0.957227 -   3.7s\n",
      "[CV] vect__ngram_range=(1, 2), vect__min_df=2, vect__stop_words=english \n",
      "[CV]  vect__ngram_range=(1, 2), vect__min_df=2, vect__stop_words=english, score=0.979351 -   3.6s\n",
      "[CV] vect__ngram_range=(1, 2), vect__min_df=2, vect__stop_words=english \n",
      "[CV]  vect__ngram_range=(1, 2), vect__min_df=2, vect__stop_words=english, score=0.967552 -   3.3s\n",
      "[CV] vect__ngram_range=(1, 2), vect__min_df=2, vect__stop_words=english \n",
      "[CV]  vect__ngram_range=(1, 2), vect__min_df=2, vect__stop_words=english, score=0.963127 -   3.1s\n",
      "[CV] vect__ngram_range=(1, 2), vect__min_df=2, vect__stop_words=english \n",
      "[CV]  vect__ngram_range=(1, 2), vect__min_df=2, vect__stop_words=english, score=0.958702 -   2.8s\n",
      "[CV] vect__ngram_range=(1, 2), vect__min_df=2, vect__stop_words=english \n",
      "[CV]  vect__ngram_range=(1, 2), vect__min_df=2, vect__stop_words=english, score=0.964602 -   3.0s\n",
      "[CV] vect__ngram_range=(1, 2), vect__min_df=2, vect__stop_words=english \n",
      "[CV]  vect__ngram_range=(1, 2), vect__min_df=2, vect__stop_words=english, score=0.958702 -   2.9s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  85 jobs       | elapsed:  4.9min\n",
      "[Parallel(n_jobs=1)]: Done  98 jobs       | elapsed:  5.7min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[CV] vect__ngram_range=(1, 2), vect__min_df=2, vect__stop_words=english \n",
      "[CV]  vect__ngram_range=(1, 2), vect__min_df=2, vect__stop_words=english, score=0.955752 -   2.9s\n",
      "[CV] vect__ngram_range=(1, 2), vect__min_df=2, vect__stop_words=english \n",
      "[CV]  vect__ngram_range=(1, 2), vect__min_df=2, vect__stop_words=english, score=0.967552 -   2.9s\n",
      "[CV] vect__ngram_range=(1, 3), vect__min_df=2, vect__stop_words=None .\n",
      "[CV]  vect__ngram_range=(1, 3), vect__min_df=2, vect__stop_words=None, score=0.929204 -   7.8s\n",
      "[CV] vect__ngram_range=(1, 3), vect__min_df=2, vect__stop_words=None .\n",
      "[CV]  vect__ngram_range=(1, 3), vect__min_df=2, vect__stop_words=None, score=0.915929 -   8.1s\n",
      "[CV] vect__ngram_range=(1, 3), vect__min_df=2, vect__stop_words=None .\n",
      "[CV]  vect__ngram_range=(1, 3), vect__min_df=2, vect__stop_words=None, score=0.943953 -   7.9s\n",
      "[CV] vect__ngram_range=(1, 3), vect__min_df=2, vect__stop_words=None .\n",
      "[CV]  vect__ngram_range=(1, 3), vect__min_df=2, vect__stop_words=None, score=0.932153 -   7.9s\n",
      "[CV] vect__ngram_range=(1, 3), vect__min_df=2, vect__stop_words=None .\n",
      "[CV]  vect__ngram_range=(1, 3), vect__min_df=2, vect__stop_words=None, score=0.936578 -   8.1s\n",
      "[CV] vect__ngram_range=(1, 3), vect__min_df=2, vect__stop_words=None .\n",
      "[CV]  vect__ngram_range=(1, 3), vect__min_df=2, vect__stop_words=None, score=0.926254 -   7.8s\n",
      "[CV] vect__ngram_range=(1, 3), vect__min_df=2, vect__stop_words=None .\n",
      "[CV]  vect__ngram_range=(1, 3), vect__min_df=2, vect__stop_words=None, score=0.932153 -   8.1s\n",
      "[CV] vect__ngram_range=(1, 3), vect__min_df=2, vect__stop_words=None .\n",
      "[CV]  vect__ngram_range=(1, 3), vect__min_df=2, vect__stop_words=None, score=0.910029 -   7.7s\n",
      "[CV] vect__ngram_range=(1, 3), vect__min_df=2, vect__stop_words=None .\n",
      "[CV]  vect__ngram_range=(1, 3), vect__min_df=2, vect__stop_words=None, score=0.915929 -   8.1s\n",
      "[CV] vect__ngram_range=(1, 3), vect__min_df=2, vect__stop_words=None .\n",
      "[CV]  vect__ngram_range=(1, 3), vect__min_df=2, vect__stop_words=None, score=0.929204 -   8.3s\n",
      "[CV] vect__ngram_range=(1, 3), vect__min_df=2, vect__stop_words=english \n",
      "[CV]  vect__ngram_range=(1, 3), vect__min_df=2, vect__stop_words=english, score=0.955752 -   6.7s\n",
      "[CV] vect__ngram_range=(1, 3), vect__min_df=2, vect__stop_words=english \n",
      "[CV]  vect__ngram_range=(1, 3), vect__min_df=2, vect__stop_words=english, score=0.958702 -   5.6s\n",
      "[CV] vect__ngram_range=(1, 3), vect__min_df=2, vect__stop_words=english \n",
      "[CV]  vect__ngram_range=(1, 3), vect__min_df=2, vect__stop_words=english, score=0.979351 -   5.1s\n",
      "[CV] vect__ngram_range=(1, 3), vect__min_df=2, vect__stop_words=english \n",
      "[CV]  vect__ngram_range=(1, 3), vect__min_df=2, vect__stop_words=english, score=0.970501 -   5.2s\n",
      "[CV] vect__ngram_range=(1, 3), vect__min_df=2, vect__stop_words=english \n",
      "[CV]  vect__ngram_range=(1, 3), vect__min_df=2, vect__stop_words=english, score=0.961652 -   5.4s\n",
      "[CV] vect__ngram_range=(1, 3), vect__min_df=2, vect__stop_words=english \n",
      "[CV]  vect__ngram_range=(1, 3), vect__min_df=2, vect__stop_words=english, score=0.958702 -   5.3s\n",
      "[CV] vect__ngram_range=(1, 3), vect__min_df=2, vect__stop_words=english \n",
      "[CV]  vect__ngram_range=(1, 3), vect__min_df=2, vect__stop_words=english, score=0.964602 -   5.8s\n",
      "[CV] vect__ngram_range=(1, 3), vect__min_df=2, vect__stop_words=english \n",
      "[CV]  vect__ngram_range=(1, 3), vect__min_df=2, vect__stop_words=english, score=0.958702 -   5.2s\n",
      "[CV] vect__ngram_range=(1, 3), vect__min_df=2, vect__stop_words=english \n",
      "[CV]  vect__ngram_range=(1, 3), vect__min_df=2, vect__stop_words=english, score=0.948378 -   5.5s\n",
      "[CV] vect__ngram_range=(1, 3), vect__min_df=2, vect__stop_words=english \n",
      "[CV]  vect__ngram_range=(1, 3), vect__min_df=2, vect__stop_words=english, score=0.966077 -   5.3s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 113 jobs       | elapsed:  7.4min\n",
      "[Parallel(n_jobs=1)]: Done 120 out of 120 | elapsed:  8.1min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "categories = ['alt.atheism', 'soc.religion.christian',\\\n",
    "              'comp.graphics', 'sci.med']\n",
    "\n",
    "twenty_train = fetch_20newsgroups(subset='train', \\\n",
    "                                  categories=categories, shuffle=True, random_state=42)\n",
    "X = twenty_train.data\n",
    "y = twenty_train.target\n",
    "\n",
    "clf = grid_search_model(create_ngram_model, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twenty_train.data[0]\n",
    "twenty_train.data[1]\n",
    "twenty_train.target[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('vect', TfidfVectorizer(analyzer='word', binary=False, decode_error=u'strict',\n",
       "        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 3), norm=u'l2', preprocessor=None, smooth_idf=True,...rue,\n",
       "        vocabulary=None)), ('clf', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "twenty_test = fetch_20newsgroups(subset='test', \\\n",
    "                                  categories=categories, shuffle=True, random_state=42)\n",
    "X_test = twenty_test.data\n",
    "y_test = twenty_test.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.905\n",
      "                        precision    recall  f1-score   support\n",
      "\n",
      "           alt.atheism       0.96      0.80      0.87       319\n",
      "soc.religion.christian       0.92      0.94      0.93       389\n",
      "         comp.graphics       0.94      0.88      0.91       396\n",
      "               sci.med       0.83      0.97      0.90       398\n",
      "\n",
      "           avg / total       0.91      0.90      0.90      1502\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "print(\"accuracy: %.3f\") % metrics.accuracy_score(y_test, pred)\n",
    "print metrics.classification_report(y_test, pred, target_names = categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Brief NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('This', 'DT'), ('is', 'VBZ'), ('a', 'DT'), ('good', 'JJ'), ('book', 'NN')]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.pos_tag(nltk.word_tokenize(\"This is a good book\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"800\"\n",
       "            height=\"600\"\n",
       "            src=\"http://www.anc.org/OANC/penn.html\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x11c6c1410>"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import IFrame\n",
    "IFrame('http://www.anc.org/OANC/penn.html', width=800, height=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment Analysis Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "emo_repl = {\n",
    "    # positive emoticons\n",
    "    \"&lt;3\": \" good \",\n",
    "    \":d\": \" good \", # :D in lower case\n",
    "    \":dd\": \" good \", # :DD in lower case\n",
    "    \"8)\": \" good \",\n",
    "    \":-)\": \" good \",\n",
    "    \":)\": \" good \",\n",
    "    \";)\": \" good \",\n",
    "    \"(-:\": \" good \",\n",
    "    \"(:\": \" good \",\n",
    "    # negative emoticons:\n",
    "    \":/\": \" bad \",\n",
    "    \":&gt;\": \" sad \",\n",
    "    \":')\": \" sad \",\n",
    "    \":-(\": \" bad \",\n",
    "    \":(\": \" bad \",\n",
    "    \":S\": \" bad \",\n",
    "    \":-S\": \" bad \",\n",
    "    }\n",
    "\n",
    "emo_repl_order = [k for (k_len,k) in reversed(sorted([(len(k),k) for k in emo_repl.keys()]))]\n",
    "\n",
    "re_repl = {\n",
    "r\"\\br\\b\": \"are\",\n",
    "r\"\\bu\\b\": \"you\",\n",
    "r\"\\bhaha\\b\": \"ha\",\n",
    "r\"\\bhahaha\\b\": \"ha\",\n",
    "r\"\\bdon't\\b\": \"do not\",\n",
    "r\"\\bdoesn't\\b\": \"does not\",\n",
    "r\"\\bdidn't\\b\": \"did not\",\n",
    "r\"\\bhasn't\\b\": \"has not\",\n",
    "r\"\\bhaven't\\b\": \"have not\",\n",
    "r\"\\bhadn't\\b\": \"had not\",\n",
    "r\"\\bwon't\\b\": \"will not\",\n",
    "r\"\\bwouldn't\\b\": \"would not\",\n",
    "r\"\\bcan't\\b\": \"can not\",\n",
    "r\"\\bcannot\\b\": \"can not\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv, collections\n",
    "import nltk\n",
    "\n",
    "def load_sent_word_net():\n",
    "    sent_scores = collections.defaultdict(list)\n",
    "    with open(\"../data/nlp/SentiWordNet_3.0.0_20130122.txt\", \"r\") as csvfile:\n",
    "        reader = csv.reader(csvfile, delimiter='\\t', quotechar=\"'\")\n",
    "    \n",
    "        for line in reader:\n",
    "            if line[0].startswith(\"#\"):\n",
    "                continue\n",
    "            if len(line) == 1:\n",
    "                continue\n",
    "            \n",
    "            #print line\n",
    "            try: \n",
    "                POS, ID, PosScore, NegScore, SynsetTerms, Gloss = line\n",
    "                if len(POS) == 0 or len(ID) == 0:\n",
    "                    continue\n",
    "                for term in SynsetTerms.split(\" \"):\n",
    "                    term = term.split(\"#\")[0]\n",
    "                    term = term.replace(\"-\", \" \").replace(\"_\", \" \")\n",
    "                    key = \"%s/%s\" % (POS, term.split(\"#\")[0])\n",
    "                    sent_scores[key].append((float(PosScore), float(NegScore)))\n",
    "            except Exception as e:\n",
    "                print \"error\"\n",
    "                \n",
    "                \n",
    "    for key, value in sent_scores.items():\n",
    "        sent_scores[key] = np.mean(value, axis = 0)\n",
    "        \n",
    "    return sent_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error\n"
     ]
    }
   ],
   "source": [
    "sent_word_net = load_sent_word_net()\n",
    "from sklearn.base import BaseEstimator\n",
    "\n",
    "class LinguisticVectorizer(BaseEstimator):\n",
    "    def get_feature_names(self):\n",
    "        return np.array(['sent_neut', 'sent_pos', 'sent_neg', 'nouns',\n",
    "                        'adjectives', 'verbs', 'adverbs', 'allcaps',\n",
    "                        'exclamation', 'question', 'hashtag', 'mentioning'])\n",
    "    \n",
    "    def fit(self, documents, y = None):\n",
    "        return self\n",
    "    \n",
    "    def _get_sentiments(self, d):\n",
    "        sent = tuple(d.split())\n",
    "        tagged = nltk.pos_tag(sent)\n",
    "        \n",
    "        pos_vals = []\n",
    "        neg_vals = []\n",
    "        \n",
    "        nouns = 0.\n",
    "        adjectives = 0.\n",
    "        verbs = 0.\n",
    "        adverbs = 0.\n",
    "        \n",
    "        for w, t in tagged:\n",
    "            p, n = 0,0\n",
    "            sent_pos_type = None\n",
    "            if t.startswith(\"NN\"):\n",
    "                sent_pos_type = \"n\"\n",
    "                nouns += 1\n",
    "            elif t.startswith(\"JJ\"):\n",
    "                sent_pos_type = \"a\"\n",
    "                adjectives += 1\n",
    "            elif t.startswith(\"VB\"):\n",
    "                sent_pos_type = \"v\"\n",
    "                verbs += 1\n",
    "            elif t.startswith(\"RB\"):\n",
    "                sent_pos_type = \"r\"\n",
    "                adverbs += 1\n",
    "                \n",
    "            if sent_pos_type is not None:\n",
    "                sent_word = \"%s/%s\" % (sent_pos_type, w)\n",
    "            \n",
    "                if sent_word in sent_word_net:\n",
    "                    p, n = sent_word_net[sent_word]\n",
    "                \n",
    "        pos_vals.append(p)\n",
    "        neg_vals.append(n)\n",
    "        l = len(sent)\n",
    "        avg_pos_val = np.mean(pos_vals)\n",
    "        avg_neg_val = np.mean(neg_vals)\n",
    "        return [1-avg_pos_val-avg_neg_val, avg_pos_val, avg_neg_val,\n",
    "               nouns/1, adjectives/1, verbs/1, adverbs/1]\n",
    "        \n",
    "    def transform(self, documents):\n",
    "        obj_val, pos_val, neg_val, nouns, adjectives, \\\n",
    "        verbs, adverbs = np.array([self._get_sentiments(d) \\\n",
    "                                  for d in documents]).T\n",
    "        \n",
    "        allcaps = []\n",
    "        exclamation = []\n",
    "        question = []\n",
    "        hastag = []\n",
    "        mentioning = []\n",
    "        \n",
    "        for d in documents:\n",
    "            allcaps.append(np.sum([t.isupper() \\\n",
    "                                  for t in d.split() if len(t) > 2]))\n",
    "            exclamation.append(d.count(\"!\"))\n",
    "            question.append(d.count(\"!\"))\n",
    "            hashtag.append(d.count(\"!\"))\n",
    "            mentioning.append(d.count(\"!\"))\n",
    "            \n",
    "        result = np.array([obj_val, pos_val, neg_val, nouns, adjectives, verbs, adverbs, allcaps,\n",
    "                          exclamation, question, hastag, mentioning]).T\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "\n",
    "def create_union_model(params = None):\n",
    "    def preprocessor(tweet):\n",
    "        tweet = tweet.lower()\n",
    "        \n",
    "        for k in emo_repl_order:\n",
    "            tweet = tweet.replace(k, emo_repl[k])\n",
    "        for r, repl in re_repl.items():\n",
    "            tweet = re.sub(r, repl, tweet)\n",
    "            \n",
    "        return tweet.replace(\"-\", \" \").replace(\"_\", \" \")\n",
    "    \n",
    "    tfidf_ngrams = TfidfVectorizer(preprocessor=preprocessor, analyzer=\"word\")\n",
    "    ling_stats = LinguisticVectorizer()\n",
    "    all_features = FeatureUnion([('ling',ling_stats), ('tfidf', tfidf_ngrams)])\n",
    "\n",
    "    clf = MultinomialNB()\n",
    "    pipeline = Pipeline([('all', all_features), ('clf', clf)])\n",
    "\n",
    "    if params:\n",
    "        pipeline.set_params(**params)\n",
    "\n",
    "    return pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 12 candidates, totalling 120 fits\n",
      "[CV] vect__ngram_range=(1, 1), vect__min_df=1, vect__stop_words=None .\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Invalid parameter vect for estimator Pipeline(steps=[('all', FeatureUnion(n_jobs=1,\n       transformer_list=[('ling', LinguisticVectorizer()), ('tfidf', TfidfVectorizer(analyzer='word', binary=False, decode_error=u'strict',\n        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',\n        lowercase=True, max_df=1.0, max_features=...    transformer_weights=None)), ('clf', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))])",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-79-bfe8e8c965c7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtwenty_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mclf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrid_search_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcreate_union_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-28-b1c4497f6002>\u001b[0m in \u001b[0;36mgrid_search_model\u001b[0;34m(clf_factory, X, Y)\u001b[0m\n\u001b[1;32m     21\u001b[0m                               \u001b[0mscore_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf1_score\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m                               verbose=10)\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0mgrid_search\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgrid_search\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_estimator_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/b3j90/anaconda/envs/nlp/lib/python2.7/site-packages/sklearn/grid_search.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    730\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    731\u001b[0m         \"\"\"\n\u001b[0;32m--> 732\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mParameterGrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    733\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    734\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/b3j90/anaconda/envs/nlp/lib/python2.7/site-packages/sklearn/grid_search.pyc\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X, y, parameter_iterable)\u001b[0m\n\u001b[1;32m    503\u001b[0m                                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_parameters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    504\u001b[0m                                     error_score=self.error_score)\n\u001b[0;32m--> 505\u001b[0;31m                 \u001b[0;32mfor\u001b[0m \u001b[0mparameters\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparameter_iterable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    506\u001b[0m                 for train, test in cv)\n\u001b[1;32m    507\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/b3j90/anaconda/envs/nlp/lib/python2.7/site-packages/sklearn/externals/joblib/parallel.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    657\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    658\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 659\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    660\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    661\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpre_dispatch\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"all\"\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mn_jobs\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/b3j90/anaconda/envs/nlp/lib/python2.7/site-packages/sklearn/externals/joblib/parallel.pyc\u001b[0m in \u001b[0;36mdispatch\u001b[0;34m(self, func, args, kwargs)\u001b[0m\n\u001b[1;32m    404\u001b[0m         \"\"\"\n\u001b[1;32m    405\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pool\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 406\u001b[0;31m             \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateApply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    407\u001b[0m             \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    408\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_verbosity_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/b3j90/anaconda/envs/nlp/lib/python2.7/site-packages/sklearn/externals/joblib/parallel.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, func, args, kwargs)\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/b3j90/anaconda/envs/nlp/lib/python2.7/site-packages/sklearn/cross_validation.pyc\u001b[0m in \u001b[0;36m_fit_and_score\u001b[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, error_score)\u001b[0m\n\u001b[1;32m   1446\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1447\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mparameters\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1448\u001b[0;31m         \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1449\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1450\u001b[0m     \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/b3j90/anaconda/envs/nlp/lib/python2.7/site-packages/sklearn/base.pyc\u001b[0m in \u001b[0;36mset_params\u001b[0;34m(self, **params)\u001b[0m\n\u001b[1;32m    247\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvalid_params\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    248\u001b[0m                     raise ValueError('Invalid parameter %s for estimator %s' %\n\u001b[0;32m--> 249\u001b[0;31m                                      (name, self))\n\u001b[0m\u001b[1;32m    250\u001b[0m                 \u001b[0msub_object\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalid_params\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m                 \u001b[0msub_object\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0msub_name\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Invalid parameter vect for estimator Pipeline(steps=[('all', FeatureUnion(n_jobs=1,\n       transformer_list=[('ling', LinguisticVectorizer()), ('tfidf', TfidfVectorizer(analyzer='word', binary=False, decode_error=u'strict',\n        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',\n        lowercase=True, max_df=1.0, max_features=...    transformer_weights=None)), ('clf', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))])"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "categories = ['alt.atheism', 'soc.religion.christian',\\\n",
    "              'comp.graphics', 'sci.med']\n",
    "\n",
    "twenty_train = fetch_20newsgroups(subset='train', \\\n",
    "                                  categories=categories, shuffle=True, random_state=42)\n",
    "X = twenty_train.data\n",
    "y = twenty_train.target\n",
    "\n",
    "clf = grid_search_model(create_union_model, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
