{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk.stem\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "english_stemmer = nltk.stem.SnowballStemmer('english')\n",
    "\n",
    "class StemmedCountVectorizer(CountVectorizer):\n",
    "    def build_analyzer(self):\n",
    "        analyzer = super(CountVectorizer, self).build_analyzer()\n",
    "        return lambda doc: (english_stemmer.stem(w) for w in analyzer(doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vectorizer = StemmedCountVectorizer(min_df=1, stop_words='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "content = [\"How to format my hard disk\", \"Hard disk format problems\"]\n",
    "X = vectorizer.fit_transform(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'disk', u'format', u'hard', u'problem']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1 1 0]\n",
      " [1 1 1 1]]\n"
     ]
    }
   ],
   "source": [
    "print(X.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "class StemmedTfidfVectorizer(TfidfVectorizer):\n",
    "    def build_analyzer(self):\n",
    "        analyzer = super(TfidfVectorizer, self).build_analyzer()\n",
    "        \n",
    "        return lambda doc: (english_stemmer.stem(w) for w in analyzer(doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vectorizer = StemmedTfidfVectorizer(min_df = 1, stop_words='english', decode_error='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'disk', u'format', u'hard', u'problem']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.fit_transform(content)\n",
    "vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The bag of words model\n",
    "\n",
    "One of the most important sub-tasks in pattern classification are **feature extraction and selection**. Some important criteria are mentioned below:\n",
    "* Salient\n",
    "* Invariant\n",
    "* Discriminatory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming and Lemmatization\n",
    "\n",
    "**Stemming** is the process of transforming a word into its root form. But, this technique can create non-real words. In contras to stemming, **lemmatization** aims to obtain canonical (grammatically correct) forms of the words, the so-called **lemmas**. Lemmatization is computationaly more difficult and expensive than stemming, and in practice, both stemming and lemmatization have little impact on the performance of text classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N-grams\n",
    "\n",
    "In the n-gram model, a token can be defined as a sequence of n items. Choose the **optimal** number of n depends on the language as well as the particular application. Examples\n",
    "* Unigram: \"El\", \"perro\", \"come\"\n",
    "* Bigram: \"El perro\", \"perro come\"\n",
    "* Trigram: \"El perro come\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag words model drawbacks\n",
    "\n",
    "* It doesn't cover word relations\n",
    "* It doesn't capture negation correctly\n",
    "* It totally fails with misspelled words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Some experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_neighbors=2, p=2, weights='uniform')\n"
     ]
    }
   ],
   "source": [
    "from sklearn import neighbors\n",
    "\n",
    "knn = neighbors.KNeighborsClassifier(n_neighbors = 2)\n",
    "print knn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn.fit([[1], [2], [3], [4], [5], [6]], [0, 0, 0, 1, 1, 1])\n",
    "knn.predict(1.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn.predict(37)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn.predict(3.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.5,  0.5]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn.predict_proba(3.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.,  0.]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn.predict_proba(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "code_match = re.compile('<pre>(.*?)</pre>',\n",
    "                       re.MULTILINE | re.DOTALL)\n",
    "link_match = re.compile('<a href=\"http://.*?\".*?>(.*?)</a>',\n",
    "                       re.MULTILINE | re.DOTALL)\n",
    "tag_match = re.compile('<[^>]*>',\n",
    "                      re.MULTILINE | re.DOTALL)\n",
    "\n",
    "def extract_features_from_body(s):\n",
    "    link_count_in_code = 0\n",
    "    # count links in code\n",
    "    for match_str in code_match.findall(s):\n",
    "        link_count_in_code += len(link_match.findall(match_str))\n",
    "        \n",
    "    return len(link_match.findall(s)) - link_count_in_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = np.asarray([extract_features_from_body(text) for post_id, text in fetch_posts() \\\n",
    "               if post_id in all_answers])\n",
    "knn = neighbors.KNeighborsClassifier()\n",
    "knn.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = np.asarray([1,2,3,4,5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import KFold\n",
    "scores = []\n",
    "\n",
    "cv = KFold(n=len(X), n_folds=10, indices=True)\n",
    "\n",
    "for train, test in cv:\n",
    "    X_train, y_train = X[train], y[train]\n",
    "    X_test, y_test = X[test], y[test]\n",
    "    clf = neighbors.KNeighborsClassifier()\n",
    "    clf.fit(X,Y)\n",
    "    scores.append(clf.score(X_test, y_test))\n",
    "    \n",
    "print \"Mean(scores)=%.5f\\tStddev(scores)=%.5f\"\\\n",
    "        % (np.mean(scores), np.std(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Designing more features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_features_from_body(s):\n",
    "    num_code_lines = 0\n",
    "    link_count_in_code = 0\n",
    "    code_free_s = s\n",
    "    \n",
    "    for match_str in code_match.findall(s):\n",
    "        num_code_lines += match_str.count('\\n')\n",
    "        code_free_s = code_match.sub(\"\", code_free_s)\n",
    "        \n",
    "        link_count_in_code += len(link_match.findall(match_str))\n",
    "    \n",
    "    links = link_match.findall(s)\n",
    "    link_count = len(links)\n",
    "    link_count -= link_count_in_code\n",
    "    html_free_s = re.sub(\" +\", \" \", \n",
    "                        tag_match.sub(\"\", code_free_s)).replace(\"\\n\",\"\")\n",
    "    link_free_s = html_free_s\n",
    "    \n",
    "    for link in links:\n",
    "        if link.lower().startswith(\"http://\"):\n",
    "            link_free_s = link_free_s.replace(link, \"\")\n",
    "   \n",
    "    num_text_tokens = html_free_s.count(\" \")\n",
    "    \n",
    "    return num_text_tokens, num_code_lines, link_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## More examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "def create_ngram_model():\n",
    "    tfidf_ngrams = TfidfVectorizer(ngram_range = (1,3),\n",
    "                                  analyzer=\"word\", binary=False)\n",
    "    clf = MultinomialNB()\n",
    "    return Pipeline([('vect', tfidf_ngrams), ('clf', clf)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve, auc\n",
    "from sklearn.cross_validation import ShuffleSplit\n",
    "\n",
    "def train_model(clf_factory, X, Y):\n",
    "    cv = ShuffleSplit(n=len(X), n_iter = 10, test_size=0.3, random_state=0)\n",
    "    \n",
    "    scores = []\n",
    "    pr_scores = []\n",
    "    \n",
    "    for train, test in cv:\n",
    "        X_train, y_train = X[train], Y[train]\n",
    "        X_test, y_test = X[test], y[test]\n",
    "        \n",
    "        clf = clf_factory()\n",
    "        clf.fit(X_train, y_train)\n",
    "        \n",
    "        train_score = clf.score(X_train, y_train)\n",
    "        test_score = clf.score(X_test, y_test)\n",
    "        \n",
    "        scores.append(test_score)\n",
    "        proba = clf.predict_proba(X_test)\n",
    "        \n",
    "        precision, recall, pr_thresholds = \\\n",
    "        precision_recall_curve(y_test, proba[:,1])\n",
    "        \n",
    "        pr_scores.append(auc(recall, precision))\n",
    "        summary = (np.mean(scores), np.std(scores), np.mean(pr_scores), np.std(pr_scores))\n",
    "        print(\"%.3f\\t%.3f\\t%.3f\\t%.3f\" % summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def grid_search_model(clf_factory, X, Y):\n",
    "    cv = ShuffleSplit(\n",
    "        n = len(X), n_iter = 10, test_size = 0.3, random_state = 0\n",
    "    )\n",
    "    \n",
    "    param_grid = dict(vect__ngram_range=[(1,1),(1,2),(1,3)],\n",
    "                      vect__min_df=[1,2],\n",
    "                      vect__stop_words=[None, \"english\"],\n",
    "                      #vect__smooth_idf = [False, True],\n",
    "                      #vect__use_idf = [False, True],\n",
    "                      #vect__sublinear_tf = [False, True],\n",
    "                      #vect__binary = [False, True]\n",
    "                     )\n",
    "    \n",
    "    grid_search = GridSearchCV(clf_factory(),\n",
    "                              param_grid,\n",
    "                              cv = cv,\n",
    "                              score_func = f1_score,\n",
    "                              verbose=10)\n",
    "    grid_search.fit(X,Y)\n",
    "    \n",
    "    return grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 12 candidates, totalling 120 fits\n",
      "[CV] vect__ngram_range=(1, 1), vect__min_df=1, vect__stop_words=None .\n",
      "[CV]  vect__ngram_range=(1, 1), vect__min_df=1, vect__stop_words=None, score=0.914454 -   0.9s\n",
      "[CV] vect__ngram_range=(1, 1), vect__min_df=1, vect__stop_words=None .\n",
      "[CV]  vect__ngram_range=(1, 1), vect__min_df=1, vect__stop_words=None, score=0.896755 -   1.0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 jobs       | elapsed:    0.9s\n",
      "[Parallel(n_jobs=1)]: Done   2 jobs       | elapsed:    2.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[CV] vect__ngram_range=(1, 1), vect__min_df=1, vect__stop_words=None .\n",
      "[CV]  vect__ngram_range=(1, 1), vect__min_df=1, vect__stop_words=None, score=0.935103 -   1.1s\n",
      "[CV] vect__ngram_range=(1, 1), vect__min_df=1, vect__stop_words=None .\n",
      "[CV]  vect__ngram_range=(1, 1), vect__min_df=1, vect__stop_words=None, score=0.904130 -   0.9s\n",
      "[CV] vect__ngram_range=(1, 1), vect__min_df=1, vect__stop_words=None .\n",
      "[CV]  vect__ngram_range=(1, 1), vect__min_df=1, vect__stop_words=None, score=0.914454 -   1.1s\n",
      "[CV] vect__ngram_range=(1, 1), vect__min_df=1, vect__stop_words=None .\n",
      "[CV]  vect__ngram_range=(1, 1), vect__min_df=1, vect__stop_words=None, score=0.892330 -   1.0s\n",
      "[CV] vect__ngram_range=(1, 1), vect__min_df=1, vect__stop_words=None .\n",
      "[CV]  vect__ngram_range=(1, 1), vect__min_df=1, vect__stop_words=None, score=0.911504 -   1.0s\n",
      "[CV] vect__ngram_range=(1, 1), vect__min_df=1, vect__stop_words=None .\n",
      "[CV]  vect__ngram_range=(1, 1), vect__min_df=1, vect__stop_words=None, score=0.882006 -   1.3s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   5 jobs       | elapsed:    5.1s\n",
      "[Parallel(n_jobs=1)]: Done   8 jobs       | elapsed:    8.4s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[CV] vect__ngram_range=(1, 1), vect__min_df=1, vect__stop_words=None .\n",
      "[CV]  vect__ngram_range=(1, 1), vect__min_df=1, vect__stop_words=None, score=0.910029 -   1.6s\n",
      "[CV] vect__ngram_range=(1, 1), vect__min_df=1, vect__stop_words=None .\n",
      "[CV]  vect__ngram_range=(1, 1), vect__min_df=1, vect__stop_words=None, score=0.907080 -   1.7s\n",
      "[CV] vect__ngram_range=(1, 1), vect__min_df=1, vect__stop_words=english \n",
      "[CV]  vect__ngram_range=(1, 1), vect__min_df=1, vect__stop_words=english, score=0.952802 -   1.5s\n",
      "[CV] vect__ngram_range=(1, 1), vect__min_df=1, vect__stop_words=english \n",
      "[CV]  vect__ngram_range=(1, 1), vect__min_df=1, vect__stop_words=english, score=0.949853 -   1.2s\n",
      "[CV] vect__ngram_range=(1, 1), vect__min_df=1, vect__stop_words=english \n",
      "[CV]  vect__ngram_range=(1, 1), vect__min_df=1, vect__stop_words=english, score=0.974926 -   1.1s\n",
      "[CV] vect__ngram_range=(1, 1), vect__min_df=1, vect__stop_words=english \n",
      "[CV]  vect__ngram_range=(1, 1), vect__min_df=1, vect__stop_words=english, score=0.964602 -   1.0s\n",
      "[CV] vect__ngram_range=(1, 1), vect__min_df=1, vect__stop_words=english \n",
      "[CV]  vect__ngram_range=(1, 1), vect__min_df=1, vect__stop_words=english, score=0.948378 -   1.3s\n",
      "[CV] vect__ngram_range=(1, 1), vect__min_df=1, vect__stop_words=english \n",
      "[CV]  vect__ngram_range=(1, 1), vect__min_df=1, vect__stop_words=english, score=0.949853 -   1.0s\n",
      "[CV] vect__ngram_range=(1, 1), vect__min_df=1, vect__stop_words=english \n",
      "[CV]  vect__ngram_range=(1, 1), vect__min_df=1, vect__stop_words=english, score=0.960177 -   0.9s\n",
      "[CV] vect__ngram_range=(1, 1), vect__min_df=1, vect__stop_words=english \n",
      "[CV]  vect__ngram_range=(1, 1), vect__min_df=1, vect__stop_words=english, score=0.948378 -   1.3s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  13 jobs       | elapsed:   15.5s\n",
      "[Parallel(n_jobs=1)]: Done  18 jobs       | elapsed:   21.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[CV] vect__ngram_range=(1, 1), vect__min_df=1, vect__stop_words=english \n",
      "[CV]  vect__ngram_range=(1, 1), vect__min_df=1, vect__stop_words=english, score=0.955752 -   1.2s\n",
      "[CV] vect__ngram_range=(1, 1), vect__min_df=1, vect__stop_words=english \n",
      "[CV]  vect__ngram_range=(1, 1), vect__min_df=1, vect__stop_words=english, score=0.958702 -   1.4s\n",
      "[CV] vect__ngram_range=(1, 2), vect__min_df=1, vect__stop_words=None .\n",
      "[CV]  vect__ngram_range=(1, 2), vect__min_df=1, vect__stop_words=None, score=0.911504 -   5.2s\n",
      "[CV] vect__ngram_range=(1, 2), vect__min_df=1, vect__stop_words=None .\n",
      "[CV]  vect__ngram_range=(1, 2), vect__min_df=1, vect__stop_words=None, score=0.896755 -   4.3s\n",
      "[CV] vect__ngram_range=(1, 2), vect__min_df=1, vect__stop_words=None .\n",
      "[CV]  vect__ngram_range=(1, 2), vect__min_df=1, vect__stop_words=None, score=0.924779 -   4.2s\n",
      "[CV] vect__ngram_range=(1, 2), vect__min_df=1, vect__stop_words=None .\n",
      "[CV]  vect__ngram_range=(1, 2), vect__min_df=1, vect__stop_words=None, score=0.907080 -   4.8s\n",
      "[CV] vect__ngram_range=(1, 2), vect__min_df=1, vect__stop_words=None .\n",
      "[CV]  vect__ngram_range=(1, 2), vect__min_df=1, vect__stop_words=None, score=0.901180 -   4.6s\n",
      "[CV] vect__ngram_range=(1, 2), vect__min_df=1, vect__stop_words=None .\n",
      "[CV]  vect__ngram_range=(1, 2), vect__min_df=1, vect__stop_words=None, score=0.883481 -   4.3s\n",
      "[CV] vect__ngram_range=(1, 2), vect__min_df=1, vect__stop_words=None .\n",
      "[CV]  vect__ngram_range=(1, 2), vect__min_df=1, vect__stop_words=None, score=0.908555 -   5.4s\n",
      "[CV] vect__ngram_range=(1, 2), vect__min_df=1, vect__stop_words=None .\n",
      "[CV]  vect__ngram_range=(1, 2), vect__min_df=1, vect__stop_words=None, score=0.880531 -   4.6s\n",
      "[CV] vect__ngram_range=(1, 2), vect__min_df=1, vect__stop_words=None .\n",
      "[CV]  vect__ngram_range=(1, 2), vect__min_df=1, vect__stop_words=None, score=0.905605 -   5.0s\n",
      "[CV] vect__ngram_range=(1, 2), vect__min_df=1, vect__stop_words=None .\n",
      "[CV]  vect__ngram_range=(1, 2), vect__min_df=1, vect__stop_words=None, score=0.899705 -   5.2s\n",
      "[CV] vect__ngram_range=(1, 2), vect__min_df=1, vect__stop_words=english \n",
      "[CV]  vect__ngram_range=(1, 2), vect__min_df=1, vect__stop_words=english, score=0.955752 -   3.7s\n",
      "[CV] vect__ngram_range=(1, 2), vect__min_df=1, vect__stop_words=english \n",
      "[CV]  vect__ngram_range=(1, 2), vect__min_df=1, vect__stop_words=english, score=0.958702 -   3.4s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  25 jobs       | elapsed:   47.1s\n",
      "[Parallel(n_jobs=1)]: Done  32 jobs       | elapsed:  1.3min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[CV] vect__ngram_range=(1, 2), vect__min_df=1, vect__stop_words=english \n",
      "[CV]  vect__ngram_range=(1, 2), vect__min_df=1, vect__stop_words=english, score=0.979351 -   3.6s\n",
      "[CV] vect__ngram_range=(1, 2), vect__min_df=1, vect__stop_words=english \n",
      "[CV]  vect__ngram_range=(1, 2), vect__min_df=1, vect__stop_words=english, score=0.969027 -   3.7s\n",
      "[CV] vect__ngram_range=(1, 2), vect__min_df=1, vect__stop_words=english \n",
      "[CV]  vect__ngram_range=(1, 2), vect__min_df=1, vect__stop_words=english, score=0.954277 -   3.8s\n",
      "[CV] vect__ngram_range=(1, 2), vect__min_df=1, vect__stop_words=english \n",
      "[CV]  vect__ngram_range=(1, 2), vect__min_df=1, vect__stop_words=english, score=0.955752 -   3.8s\n",
      "[CV] vect__ngram_range=(1, 2), vect__min_df=1, vect__stop_words=english \n",
      "[CV]  vect__ngram_range=(1, 2), vect__min_df=1, vect__stop_words=english, score=0.966077 -   3.2s\n",
      "[CV] vect__ngram_range=(1, 2), vect__min_df=1, vect__stop_words=english \n",
      "[CV]  vect__ngram_range=(1, 2), vect__min_df=1, vect__stop_words=english, score=0.957227 -   3.0s\n",
      "[CV] vect__ngram_range=(1, 2), vect__min_df=1, vect__stop_words=english \n",
      "[CV]  vect__ngram_range=(1, 2), vect__min_df=1, vect__stop_words=english, score=0.955752 -   3.0s\n",
      "[CV] vect__ngram_range=(1, 2), vect__min_df=1, vect__stop_words=english \n",
      "[CV]  vect__ngram_range=(1, 2), vect__min_df=1, vect__stop_words=english, score=0.963127 -   3.2s\n",
      "[CV] vect__ngram_range=(1, 3), vect__min_df=1, vect__stop_words=None .\n",
      "[CV]  vect__ngram_range=(1, 3), vect__min_df=1, vect__stop_words=None, score=0.924779 -   8.2s\n",
      "[CV] vect__ngram_range=(1, 3), vect__min_df=1, vect__stop_words=None .\n",
      "[CV]  vect__ngram_range=(1, 3), vect__min_df=1, vect__stop_words=None, score=0.910029 -   8.6s\n",
      "[CV] vect__ngram_range=(1, 3), vect__min_df=1, vect__stop_words=None .\n",
      "[CV]  vect__ngram_range=(1, 3), vect__min_df=1, vect__stop_words=None, score=0.936578 -   8.6s\n",
      "[CV] vect__ngram_range=(1, 3), vect__min_df=1, vect__stop_words=None .\n",
      "[CV]  vect__ngram_range=(1, 3), vect__min_df=1, vect__stop_words=None, score=0.921829 -   8.7s\n",
      "[CV] vect__ngram_range=(1, 3), vect__min_df=1, vect__stop_words=None .\n",
      "[CV]  vect__ngram_range=(1, 3), vect__min_df=1, vect__stop_words=None, score=0.915929 -   9.1s\n",
      "[CV] vect__ngram_range=(1, 3), vect__min_df=1, vect__stop_words=None .\n",
      "[CV]  vect__ngram_range=(1, 3), vect__min_df=1, vect__stop_words=None, score=0.904130 -   8.5s\n",
      "[CV] vect__ngram_range=(1, 3), vect__min_df=1, vect__stop_words=None .\n",
      "[CV]  vect__ngram_range=(1, 3), vect__min_df=1, vect__stop_words=None, score=0.921829 -   8.6s\n",
      "[CV] vect__ngram_range=(1, 3), vect__min_df=1, vect__stop_words=None .\n",
      "[CV]  vect__ngram_range=(1, 3), vect__min_df=1, vect__stop_words=None, score=0.890855 -   8.4s\n",
      "[CV] vect__ngram_range=(1, 3), vect__min_df=1, vect__stop_words=None .\n",
      "[CV]  vect__ngram_range=(1, 3), vect__min_df=1, vect__stop_words=None, score=0.911504 -   8.4s\n",
      "[CV] vect__ngram_range=(1, 3), vect__min_df=1, vect__stop_words=None .\n",
      "[CV]  vect__ngram_range=(1, 3), vect__min_df=1, vect__stop_words=None, score=0.920354 -   8.4s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  41 jobs       | elapsed:  1.9min\n",
      "[Parallel(n_jobs=1)]: Done  50 jobs       | elapsed:  3.2min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[CV] vect__ngram_range=(1, 3), vect__min_df=1, vect__stop_words=english \n",
      "[CV]  vect__ngram_range=(1, 3), vect__min_df=1, vect__stop_words=english, score=0.954277 -   5.3s\n",
      "[CV] vect__ngram_range=(1, 3), vect__min_df=1, vect__stop_words=english \n",
      "[CV]  vect__ngram_range=(1, 3), vect__min_df=1, vect__stop_words=english, score=0.964602 -   5.5s\n",
      "[CV] vect__ngram_range=(1, 3), vect__min_df=1, vect__stop_words=english \n",
      "[CV]  vect__ngram_range=(1, 3), vect__min_df=1, vect__stop_words=english, score=0.979351 -   7.2s\n",
      "[CV] vect__ngram_range=(1, 3), vect__min_df=1, vect__stop_words=english \n",
      "[CV]  vect__ngram_range=(1, 3), vect__min_df=1, vect__stop_words=english, score=0.973451 -   5.9s\n",
      "[CV] vect__ngram_range=(1, 3), vect__min_df=1, vect__stop_words=english \n",
      "[CV]  vect__ngram_range=(1, 3), vect__min_df=1, vect__stop_words=english, score=0.961652 -   5.7s\n",
      "[CV] vect__ngram_range=(1, 3), vect__min_df=1, vect__stop_words=english \n",
      "[CV]  vect__ngram_range=(1, 3), vect__min_df=1, vect__stop_words=english, score=0.958702 -   5.6s\n",
      "[CV] vect__ngram_range=(1, 3), vect__min_df=1, vect__stop_words=english \n",
      "[CV]  vect__ngram_range=(1, 3), vect__min_df=1, vect__stop_words=english, score=0.969027 -   5.4s\n",
      "[CV] vect__ngram_range=(1, 3), vect__min_df=1, vect__stop_words=english \n",
      "[CV]  vect__ngram_range=(1, 3), vect__min_df=1, vect__stop_words=english, score=0.961652 -   6.8s\n",
      "[CV] vect__ngram_range=(1, 3), vect__min_df=1, vect__stop_words=english \n",
      "[CV]  vect__ngram_range=(1, 3), vect__min_df=1, vect__stop_words=english, score=0.960177 -   6.6s\n",
      "[CV] vect__ngram_range=(1, 3), vect__min_df=1, vect__stop_words=english \n",
      "[CV]  vect__ngram_range=(1, 3), vect__min_df=1, vect__stop_words=english, score=0.967552 -   5.5s\n",
      "[CV] vect__ngram_range=(1, 1), vect__min_df=2, vect__stop_words=None .\n",
      "[CV]  vect__ngram_range=(1, 1), vect__min_df=2, vect__stop_words=None, score=0.920354 -   1.0s\n",
      "[CV] vect__ngram_range=(1, 1), vect__min_df=2, vect__stop_words=None .\n",
      "[CV]  vect__ngram_range=(1, 1), vect__min_df=2, vect__stop_words=None, score=0.911504 -   1.0s\n",
      "[CV] vect__ngram_range=(1, 1), vect__min_df=2, vect__stop_words=None .\n",
      "[CV]  vect__ngram_range=(1, 1), vect__min_df=2, vect__stop_words=None, score=0.942478 -   1.1s\n",
      "[CV] vect__ngram_range=(1, 1), vect__min_df=2, vect__stop_words=None .\n",
      "[CV]  vect__ngram_range=(1, 1), vect__min_df=2, vect__stop_words=None, score=0.918879 -   0.9s\n",
      "[CV] vect__ngram_range=(1, 1), vect__min_df=2, vect__stop_words=None .\n",
      "[CV]  vect__ngram_range=(1, 1), vect__min_df=2, vect__stop_words=None, score=0.923304 -   1.1s\n",
      "[CV] vect__ngram_range=(1, 1), vect__min_df=2, vect__stop_words=None .\n",
      "[CV]  vect__ngram_range=(1, 1), vect__min_df=2, vect__stop_words=None, score=0.915929 -   1.0s\n",
      "[CV] vect__ngram_range=(1, 1), vect__min_df=2, vect__stop_words=None .\n",
      "[CV]  vect__ngram_range=(1, 1), vect__min_df=2, vect__stop_words=None, score=0.929204 -   1.0s\n",
      "[CV] vect__ngram_range=(1, 1), vect__min_df=2, vect__stop_words=None .\n",
      "[CV]  vect__ngram_range=(1, 1), vect__min_df=2, vect__stop_words=None, score=0.911504 -   1.1s\n",
      "[CV] vect__ngram_range=(1, 1), vect__min_df=2, vect__stop_words=None .\n",
      "[CV]  vect__ngram_range=(1, 1), vect__min_df=2, vect__stop_words=None, score=0.920354 -   1.1s\n",
      "[CV] vect__ngram_range=(1, 1), vect__min_df=2, vect__stop_words=None .\n",
      "[CV]  vect__ngram_range=(1, 1), vect__min_df=2, vect__stop_words=None, score=0.923304 -   1.0s\n",
      "[CV] vect__ngram_range=(1, 1), vect__min_df=2, vect__stop_words=english \n",
      "[CV]  vect__ngram_range=(1, 1), vect__min_df=2, vect__stop_words=english, score=0.958702 -   1.1s\n",
      "[CV] vect__ngram_range=(1, 1), vect__min_df=2, vect__stop_words=english \n",
      "[CV]  vect__ngram_range=(1, 1), vect__min_df=2, vect__stop_words=english, score=0.955752 -   1.0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  61 jobs       | elapsed:  4.3min\n",
      "[Parallel(n_jobs=1)]: Done  72 jobs       | elapsed:  4.5min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[CV] vect__ngram_range=(1, 1), vect__min_df=2, vect__stop_words=english \n",
      "[CV]  vect__ngram_range=(1, 1), vect__min_df=2, vect__stop_words=english, score=0.973451 -   1.0s\n",
      "[CV] vect__ngram_range=(1, 1), vect__min_df=2, vect__stop_words=english \n",
      "[CV]  vect__ngram_range=(1, 1), vect__min_df=2, vect__stop_words=english, score=0.971976 -   1.0s\n",
      "[CV] vect__ngram_range=(1, 1), vect__min_df=2, vect__stop_words=english \n",
      "[CV]  vect__ngram_range=(1, 1), vect__min_df=2, vect__stop_words=english, score=0.954277 -   1.0s\n",
      "[CV] vect__ngram_range=(1, 1), vect__min_df=2, vect__stop_words=english \n",
      "[CV]  vect__ngram_range=(1, 1), vect__min_df=2, vect__stop_words=english, score=0.958702 -   0.9s\n",
      "[CV] vect__ngram_range=(1, 1), vect__min_df=2, vect__stop_words=english \n",
      "[CV]  vect__ngram_range=(1, 1), vect__min_df=2, vect__stop_words=english, score=0.961652 -   1.1s\n",
      "[CV] vect__ngram_range=(1, 1), vect__min_df=2, vect__stop_words=english \n",
      "[CV]  vect__ngram_range=(1, 1), vect__min_df=2, vect__stop_words=english, score=0.955752 -   1.0s\n",
      "[CV] vect__ngram_range=(1, 1), vect__min_df=2, vect__stop_words=english \n",
      "[CV]  vect__ngram_range=(1, 1), vect__min_df=2, vect__stop_words=english, score=0.957227 -   0.9s\n",
      "[CV] vect__ngram_range=(1, 1), vect__min_df=2, vect__stop_words=english \n",
      "[CV]  vect__ngram_range=(1, 1), vect__min_df=2, vect__stop_words=english, score=0.958702 -   1.0s\n",
      "[CV] vect__ngram_range=(1, 2), vect__min_df=2, vect__stop_words=None .\n",
      "[CV]  vect__ngram_range=(1, 2), vect__min_df=2, vect__stop_words=None, score=0.923304 -   3.9s\n",
      "[CV] vect__ngram_range=(1, 2), vect__min_df=2, vect__stop_words=None .\n",
      "[CV]  vect__ngram_range=(1, 2), vect__min_df=2, vect__stop_words=None, score=0.911504 -   3.9s\n",
      "[CV] vect__ngram_range=(1, 2), vect__min_df=2, vect__stop_words=None .\n",
      "[CV]  vect__ngram_range=(1, 2), vect__min_df=2, vect__stop_words=None, score=0.939528 -   4.0s\n",
      "[CV] vect__ngram_range=(1, 2), vect__min_df=2, vect__stop_words=None .\n",
      "[CV]  vect__ngram_range=(1, 2), vect__min_df=2, vect__stop_words=None, score=0.924779 -   4.0s\n",
      "[CV] vect__ngram_range=(1, 2), vect__min_df=2, vect__stop_words=None .\n",
      "[CV]  vect__ngram_range=(1, 2), vect__min_df=2, vect__stop_words=None, score=0.923304 -   3.9s\n",
      "[CV] vect__ngram_range=(1, 2), vect__min_df=2, vect__stop_words=None .\n",
      "[CV]  vect__ngram_range=(1, 2), vect__min_df=2, vect__stop_words=None, score=0.917404 -   3.8s\n",
      "[CV] vect__ngram_range=(1, 2), vect__min_df=2, vect__stop_words=None .\n",
      "[CV]  vect__ngram_range=(1, 2), vect__min_df=2, vect__stop_words=None, score=0.927729 -   4.0s\n",
      "[CV] vect__ngram_range=(1, 2), vect__min_df=2, vect__stop_words=None .\n",
      "[CV]  vect__ngram_range=(1, 2), vect__min_df=2, vect__stop_words=None, score=0.910029 -   3.8s\n",
      "[CV] vect__ngram_range=(1, 2), vect__min_df=2, vect__stop_words=None .\n",
      "[CV]  vect__ngram_range=(1, 2), vect__min_df=2, vect__stop_words=None, score=0.915929 -   3.8s\n",
      "[CV] vect__ngram_range=(1, 2), vect__min_df=2, vect__stop_words=None .\n",
      "[CV]  vect__ngram_range=(1, 2), vect__min_df=2, vect__stop_words=None, score=0.921829 -   4.0s\n",
      "[CV] vect__ngram_range=(1, 2), vect__min_df=2, vect__stop_words=english \n",
      "[CV]  vect__ngram_range=(1, 2), vect__min_df=2, vect__stop_words=english, score=0.955752 -   4.0s\n",
      "[CV] vect__ngram_range=(1, 2), vect__min_df=2, vect__stop_words=english \n",
      "[CV]  vect__ngram_range=(1, 2), vect__min_df=2, vect__stop_words=english, score=0.957227 -   3.7s\n",
      "[CV] vect__ngram_range=(1, 2), vect__min_df=2, vect__stop_words=english \n",
      "[CV]  vect__ngram_range=(1, 2), vect__min_df=2, vect__stop_words=english, score=0.979351 -   3.6s\n",
      "[CV] vect__ngram_range=(1, 2), vect__min_df=2, vect__stop_words=english \n",
      "[CV]  vect__ngram_range=(1, 2), vect__min_df=2, vect__stop_words=english, score=0.967552 -   3.3s\n",
      "[CV] vect__ngram_range=(1, 2), vect__min_df=2, vect__stop_words=english \n",
      "[CV]  vect__ngram_range=(1, 2), vect__min_df=2, vect__stop_words=english, score=0.963127 -   3.1s\n",
      "[CV] vect__ngram_range=(1, 2), vect__min_df=2, vect__stop_words=english \n",
      "[CV]  vect__ngram_range=(1, 2), vect__min_df=2, vect__stop_words=english, score=0.958702 -   2.8s\n",
      "[CV] vect__ngram_range=(1, 2), vect__min_df=2, vect__stop_words=english \n",
      "[CV]  vect__ngram_range=(1, 2), vect__min_df=2, vect__stop_words=english, score=0.964602 -   3.0s\n",
      "[CV] vect__ngram_range=(1, 2), vect__min_df=2, vect__stop_words=english \n",
      "[CV]  vect__ngram_range=(1, 2), vect__min_df=2, vect__stop_words=english, score=0.958702 -   2.9s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  85 jobs       | elapsed:  4.9min\n",
      "[Parallel(n_jobs=1)]: Done  98 jobs       | elapsed:  5.7min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[CV] vect__ngram_range=(1, 2), vect__min_df=2, vect__stop_words=english \n",
      "[CV]  vect__ngram_range=(1, 2), vect__min_df=2, vect__stop_words=english, score=0.955752 -   2.9s\n",
      "[CV] vect__ngram_range=(1, 2), vect__min_df=2, vect__stop_words=english \n",
      "[CV]  vect__ngram_range=(1, 2), vect__min_df=2, vect__stop_words=english, score=0.967552 -   2.9s\n",
      "[CV] vect__ngram_range=(1, 3), vect__min_df=2, vect__stop_words=None .\n",
      "[CV]  vect__ngram_range=(1, 3), vect__min_df=2, vect__stop_words=None, score=0.929204 -   7.8s\n",
      "[CV] vect__ngram_range=(1, 3), vect__min_df=2, vect__stop_words=None .\n",
      "[CV]  vect__ngram_range=(1, 3), vect__min_df=2, vect__stop_words=None, score=0.915929 -   8.1s\n",
      "[CV] vect__ngram_range=(1, 3), vect__min_df=2, vect__stop_words=None .\n",
      "[CV]  vect__ngram_range=(1, 3), vect__min_df=2, vect__stop_words=None, score=0.943953 -   7.9s\n",
      "[CV] vect__ngram_range=(1, 3), vect__min_df=2, vect__stop_words=None .\n",
      "[CV]  vect__ngram_range=(1, 3), vect__min_df=2, vect__stop_words=None, score=0.932153 -   7.9s\n",
      "[CV] vect__ngram_range=(1, 3), vect__min_df=2, vect__stop_words=None .\n",
      "[CV]  vect__ngram_range=(1, 3), vect__min_df=2, vect__stop_words=None, score=0.936578 -   8.1s\n",
      "[CV] vect__ngram_range=(1, 3), vect__min_df=2, vect__stop_words=None .\n",
      "[CV]  vect__ngram_range=(1, 3), vect__min_df=2, vect__stop_words=None, score=0.926254 -   7.8s\n",
      "[CV] vect__ngram_range=(1, 3), vect__min_df=2, vect__stop_words=None .\n",
      "[CV]  vect__ngram_range=(1, 3), vect__min_df=2, vect__stop_words=None, score=0.932153 -   8.1s\n",
      "[CV] vect__ngram_range=(1, 3), vect__min_df=2, vect__stop_words=None .\n",
      "[CV]  vect__ngram_range=(1, 3), vect__min_df=2, vect__stop_words=None, score=0.910029 -   7.7s\n",
      "[CV] vect__ngram_range=(1, 3), vect__min_df=2, vect__stop_words=None .\n",
      "[CV]  vect__ngram_range=(1, 3), vect__min_df=2, vect__stop_words=None, score=0.915929 -   8.1s\n",
      "[CV] vect__ngram_range=(1, 3), vect__min_df=2, vect__stop_words=None .\n",
      "[CV]  vect__ngram_range=(1, 3), vect__min_df=2, vect__stop_words=None, score=0.929204 -   8.3s\n",
      "[CV] vect__ngram_range=(1, 3), vect__min_df=2, vect__stop_words=english \n",
      "[CV]  vect__ngram_range=(1, 3), vect__min_df=2, vect__stop_words=english, score=0.955752 -   6.7s\n",
      "[CV] vect__ngram_range=(1, 3), vect__min_df=2, vect__stop_words=english \n",
      "[CV]  vect__ngram_range=(1, 3), vect__min_df=2, vect__stop_words=english, score=0.958702 -   5.6s\n",
      "[CV] vect__ngram_range=(1, 3), vect__min_df=2, vect__stop_words=english \n",
      "[CV]  vect__ngram_range=(1, 3), vect__min_df=2, vect__stop_words=english, score=0.979351 -   5.1s\n",
      "[CV] vect__ngram_range=(1, 3), vect__min_df=2, vect__stop_words=english \n",
      "[CV]  vect__ngram_range=(1, 3), vect__min_df=2, vect__stop_words=english, score=0.970501 -   5.2s\n",
      "[CV] vect__ngram_range=(1, 3), vect__min_df=2, vect__stop_words=english \n",
      "[CV]  vect__ngram_range=(1, 3), vect__min_df=2, vect__stop_words=english, score=0.961652 -   5.4s\n",
      "[CV] vect__ngram_range=(1, 3), vect__min_df=2, vect__stop_words=english \n",
      "[CV]  vect__ngram_range=(1, 3), vect__min_df=2, vect__stop_words=english, score=0.958702 -   5.3s\n",
      "[CV] vect__ngram_range=(1, 3), vect__min_df=2, vect__stop_words=english \n",
      "[CV]  vect__ngram_range=(1, 3), vect__min_df=2, vect__stop_words=english, score=0.964602 -   5.8s\n",
      "[CV] vect__ngram_range=(1, 3), vect__min_df=2, vect__stop_words=english \n",
      "[CV]  vect__ngram_range=(1, 3), vect__min_df=2, vect__stop_words=english, score=0.958702 -   5.2s\n",
      "[CV] vect__ngram_range=(1, 3), vect__min_df=2, vect__stop_words=english \n",
      "[CV]  vect__ngram_range=(1, 3), vect__min_df=2, vect__stop_words=english, score=0.948378 -   5.5s\n",
      "[CV] vect__ngram_range=(1, 3), vect__min_df=2, vect__stop_words=english \n",
      "[CV]  vect__ngram_range=(1, 3), vect__min_df=2, vect__stop_words=english, score=0.966077 -   5.3s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 113 jobs       | elapsed:  7.4min\n",
      "[Parallel(n_jobs=1)]: Done 120 out of 120 | elapsed:  8.1min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "categories = ['alt.atheism', 'soc.religion.christian',\\\n",
    "              'comp.graphics', 'sci.med']\n",
    "\n",
    "twenty_train = fetch_20newsgroups(subset='train', \\\n",
    "                                  categories=categories, shuffle=True, random_state=42)\n",
    "X = twenty_train.data\n",
    "y = twenty_train.target\n",
    "\n",
    "clf = grid_search_model(create_ngram_model, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twenty_train.data[0]\n",
    "twenty_train.data[1]\n",
    "twenty_train.target[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('vect', TfidfVectorizer(analyzer='word', binary=False, decode_error=u'strict',\n",
       "        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 3), norm=u'l2', preprocessor=None, smooth_idf=True,...rue,\n",
       "        vocabulary=None)), ('clf', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "twenty_test = fetch_20newsgroups(subset='test', \\\n",
    "                                  categories=categories, shuffle=True, random_state=42)\n",
    "X_test = twenty_test.data\n",
    "y_test = twenty_test.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.905\n",
      "                        precision    recall  f1-score   support\n",
      "\n",
      "           alt.atheism       0.96      0.80      0.87       319\n",
      "soc.religion.christian       0.92      0.94      0.93       389\n",
      "         comp.graphics       0.94      0.88      0.91       396\n",
      "               sci.med       0.83      0.97      0.90       398\n",
      "\n",
      "           avg / total       0.91      0.90      0.90      1502\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "print(\"accuracy: %.3f\") % metrics.accuracy_score(y_test, pred)\n",
    "print metrics.classification_report(y_test, pred, target_names = categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment Analysis Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
