{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic regression\n",
    "\n",
    "Regression means that we try to find the best-fit set parameters. Finsing the best-fit is similar to regression, and in this method it's how we train our classifier. Optimization algorithms are used to find the parametres. Logistic regression, instead of its name, is a classification model that is very easy to implement but performs very well on linearly separable classes. It is one of the most widely used algorthms for classification in industry. \n",
    "\n",
    "The logistic regression \n",
    "\n",
    "**Pros**\n",
    "Computationally inexpensive, easy to implement, knowledge representation easy to interpret.\n",
    "**Cons**\n",
    "Prone to underfitting, may have low accuracy.\n",
    "**Works with**\n",
    "Numeric values, nominal values\n",
    "\n",
    "For the logistic regression classifier we 'll our features and multiply each one by a weight and then add them up. This result will be put into the sigmoid, and we'll get a number between 0 and 1. Anything above 0.5 we'll classify as a 1, and anything below 0.5 we'll classify as a 0. You can also think as a logistic regression as a probability estimate.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loadDataSet():\n",
    "    dataMat = []\n",
    "    labelMat = []\n",
    "    fr = open('testSet.txt')\n",
    "    for line in fr.readlines():\n",
    "        lineArr = line.strip().split()\n",
    "        dataMat.append([1.0, floar(lineArr[0]), float(lineArr[1])])\n",
    "        labelMat.append(int(lineArr[2]))\n",
    "    return dataMat, labelMat\n",
    "\n",
    "def sigmoid(inX):\n",
    "    return 1.0/(1+exp(-inX))\n",
    "\n",
    "def gradAscent(dataMatIn, classLabels):\n",
    "    dataMatrix = mat(dataMatIn)\n",
    "    labelMat = mat(classLabels).traspose()\n",
    "    m, n = shape(dataMatrix)\n",
    "    alpha = 0.001\n",
    "    maxCycles = 500\n",
    "    weights = ones((n,1))\n",
    "    for k in range(maxCycles):\n",
    "        h = sigmoid(dataMatrix * weights)\n",
    "        error = (labelMat - h)\n",
    "        weights = weights + alpha * dataMatrix.traspose() * error\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stochastic Gradient Ascent\n",
    "\n",
    "It is an example of an online learning algorithm. This is known as online because we can incrementally update the classifier as new data comes in rather thatn all at once. The all-at-once is known as **batch processing**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def stocGradAscent0(dataMatrix, classLabels):\n",
    "    m,n = shape(dataMatrix)\n",
    "    alpha = 0.01\n",
    "    weights = ones(n)\n",
    "    for i in range(m):\n",
    "        h = sigmoid(sum(dataMatrix[i]*weights))\n",
    "        error = classLabels[i] -h\n",
    "        weights = weights + alpha * error * dataMatrix[i]\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def stocGradAscent1(dataMatrix, classLabels, numIter = 150):\n",
    "    m,n = shape(dataMatrix)\n",
    "    weights = ones(n)\n",
    "    for j in range(numIter):\n",
    "        dataIndex = range(m)\n",
    "        for i in range(m):\n",
    "            alpha = 4/(1.0+j+i)+0.01\n",
    "            randIndex = int(random.uniform(0,len(dataIndex)))\n",
    "            h = sigmoid(sum(dataMatrix[randIndex]*weights))\n",
    "            error = classLabels[randIndex] - h\n",
    "            weights = weights + alpha * error * dataMatrix[randIndex]\n",
    "            del(dataIndex[randIndex])\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
