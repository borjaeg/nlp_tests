{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear regression\n",
    "\n",
    "We start formulating a model form some data by first fitting a straight line (that is an equation) with some parameters and coefficients over this data. We can think of linear regression as a way of lifing a straight line **y=mx+c**, over the sample data, if we assume that the sample data has only a single dimension.\n",
    "\n",
    "The linear regression model is simply described as a linear eqation that represents the **regressand** or **dependent variable** of the model. The formulated regression model can have one to several parameters depending on the available data, and these parameters of the model are also termed as **regressors**, **features**, or **independent variables** of the model.\n",
    "\n",
    "Sometimes, the function creates a linear model of our data by using the **ordinaly-least squares (OLS)** curve-fitting algorithm. Once we've defined a linear model over our data, it's obvious that not all the points will be on a line that is plotted to represent the formulated model. Each data point has some deviation from the linear mode's plot. To represent the ovarall deviation of the model from some given data, we can use the **residual sum of quares**, **mean-squared error**, and **root mean-squared error functions**. The value of these three functions represent a **scalar mesaure** of the amount of error in the formulated model.\n",
    "\n",
    "The **sum of squared errors of prediction (SSE)** is simply the sum of errors in a formulated model. The SSE is also termed as **residual sum of quares (RSS)**. \n",
    "\n",
    "The **mean-squared error (MSE)** measures the average magnitude of error in a formulated model without considering the direction of the errors. We can calculate this value by squaring the differences of all the given values of the dependent variable and their corresponding predicted values on the formulated linear model, and calculating the mean of these squared errors. If the MSE of a formulated model is zero, then we can say that the model fits the given data perfectly.\n",
    "\n",
    "The **root mean-squared error (RMSE)** is simple the square root of MSE and is often used to measure the deviation of a formulated linear model.\n",
    "\n",
    "In order to formulate a model that best fits the sample data, we should strive to minimize the previously described values. For some given data, we can formulate several models and calculate the error for each model. This calculated error can then be used to determine which formulated model is the best fit for the data, thus selecting the optimal linear model for the given data.\n",
    "\n",
    "Based on the MSE of a formulated model, the model is said to have a cost function. ** The problem of fitting a linear model over some data is equivalent to the problem of minimizing the cost function of a formulated linear model**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification\n",
    "\n",
    "We are interested in the category of the observed values rather than predicting a value based on the given set of values. The independent variables of the classification model are also termed as the **explanatory variables** of the model, and the dependent variable is also called the **outcome**, **category**, or **class** of the observed values. The outcome of a classification model is always a discrete value. This is one of the primary differences between classification and regression. A **classifier** can be formally defined as a function that maps a set of values to the cateogry of a class. Similar to regression, the problem of classifying the obeserved values for the given independent variables is analogous to determining the best-fit function for the given training data.\n",
    "\n",
    "### Logistic regression\n",
    "\n",
    "Regression means that we try to find the best-fit set parameters. Finsing the best-fit is similar to regression, and in this method \n",
    "\n",
    "### Naive Bayes\n",
    "\n",
    "### K-nearest neighbor\n",
    "\n",
    "This algorithm is a form of **lazy leanrning** in which all the computation is deferred until classification. It can be applied to regression as well by simply selecteind the predicted values as the average of the nearest values of the dependent variable for a set of observed feature values. This technique of modeling regresion is, in fact, a generalization of **linear interpolation**.\n",
    "\n",
    "### Decision Trees\n",
    "\n",
    "The process of constructing a decision tree is loosely based on the concepts of **information entropy and information gain** from information theory. A decision tree is a graph that describes a model of decisions and their possible consequences. An internal node in a decision tree represents a decision, or rather a condition of a particular feature in the context of classification. It has two possible outcomes that are represented by the left and right subtrees of the node. Of course, a node in the decision tree coul also have more than two subtrees. Each leaf node represents a particular class.\n",
    "\n",
    "There are actually several algorithms that are used to construct a decision tree from some training data. Generally, the tree is constructed by splitting the set of sample values in the training data into smaller subsets based on an attribute value test. The process is repeated in each subset until splitting a given subset of sample values no longer adds internal nodes to the decision tree.\n",
    "\n",
    "Once a decision tree has been created, we can optionally perform **pruning** on the tree. Pruning is simply the process of removing any extraneous decision nodes from the tree. This can be thought as a form for the regularization of decision tree through which we prevent underfitting or overfitting of the estimated decision tree model.\n",
    "\n",
    "**J48** is an open source implementation of the **C4.5** algorithm in Java.\n",
    "\n",
    "**Pros** Computationally cheap to use, easy for humans to understand learned results, missing values OK, can deal with irrelevant features.\n",
    "**Cons**: Prone to overfitting\n",
    "**Works with**: Numeric values and  nominal values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Function to calculate the Shannon Entropy of a dataset\n",
    "from math import log\n",
    "import operator\n",
    "\n",
    "def calcShannonEnt(dataset):\n",
    "    numEntries = len(dataset)\n",
    "    labelCounts = {}\n",
    "    for featVec in dataset:\n",
    "        currentLabel = featVec[-1] \n",
    "        #print currentLabel\n",
    "        if currentLabel not in labelCounts.keys():\n",
    "            labelCounts[currentLabel] = 1\n",
    "        else:\n",
    "            labelCounts[currentLabel] += 1\n",
    "            \n",
    "    #print labelCounts\n",
    "    shannonEnt = 0.0\n",
    "    for key in labelCounts:\n",
    "        prob = float(labelCounts[key]) / numEntries\n",
    "        #print prob\n",
    "        shannonEnt -= prob * log(prob,2)\n",
    "        \n",
    "    return shannonEnt\n",
    "\n",
    "def splitDataSet(dataset, axis, value):\n",
    "    retDataSet = []\n",
    "    for featVec in dataset:\n",
    "        if featVec[axis] == value:\n",
    "            reducedFeatVec = featVec[:axis]\n",
    "            reducedFeatVec.extend(featVec[axis+1:])\n",
    "            retDataSet.append(reducedFeatVec)\n",
    "    return retDataSet\n",
    "\n",
    "def createTree(dataset, labels):\n",
    "    classList = [example[-1] for example in dataset]\n",
    "    if classList.count(classList[0]) == len(classList):\n",
    "        return classList[0]\n",
    "    if len(dataset[0]) == 1:\n",
    "        return majorityCnt(classList)\n",
    "    bestFeature = chooseBestFeatureToSplit(dataset)\n",
    "    bestFeatureLabel = labels[bestFeature]\n",
    "    myTree = {bestFeatureLabel: {}}\n",
    "    del(labels[bestFeature])\n",
    "    featValues = [example[bestFeature] for example in dataset]\n",
    "    uniqueVals = set(featValues)\n",
    "    for value in uniqueVals:\n",
    "        subLabels = labels[:]\n",
    "        myTree[bestFeatureLabel][value] = createTree(splitDataSet\\\n",
    "                                                    (dataset, bestFeature, value), subLabels)\n",
    "    return myTree\n",
    "\n",
    "def majorityCnt(classList):\n",
    "    classCount = {}\n",
    "    for vote in classList:\n",
    "        if vote not in classCount.keys():\n",
    "            classCount[vote] = 1\n",
    "        else:\n",
    "            classList[vote] +=1\n",
    "    \n",
    "\n",
    "def chooseBestFeatureToSplit(dataset):\n",
    "    numFeatures = len(dataset[0]) - 1\n",
    "    baseEntropy = calcShannonEnt(dataset)\n",
    "    bestInfoGain = 0.0\n",
    "    bestFeature = -1\n",
    "    for i in range(numFeatures):\n",
    "        featList = [example[i] for example in dataset]\n",
    "        uniqueVals = set(featList)\n",
    "        newEntropy = 0.0\n",
    "        for value in uniqueVals:\n",
    "            subDataSet = splitDataSet(dataset, i, value)\n",
    "            prob = len(subDataSet)/float(len(dataset))\n",
    "            newEntropy += prob * calcShannonEnt(subDataSet)\n",
    "\n",
    "        infoGain = baseEntropy - newEntropy\n",
    "        if (infoGain > bestInfoGain):\n",
    "            bestInfoGain = infoGain\n",
    "            bestFeature = i\n",
    "\n",
    "    return bestFeature\n",
    "\n",
    "def createDataSet():\n",
    "    dataset = [[1, 1, 'yes'],\n",
    "              [1, 1, 'yes'],\n",
    "              [1, 0, 'no'],\n",
    "              [0, 1, 'no'],\n",
    "              [0, 1, 'no'],]\n",
    "\n",
    "    labels = ['no surfacing', 'flippers']\n",
    "\n",
    "    return dataset, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "myDat, labels = createDataSet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9709505944546686"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calcShannonEnt(myDat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.3709505944546687"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myDat[0][-1] = 'maybe'\n",
    "calcShannonEnt(myDat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 'maybe'], [1, 'yes'], [0, 'no']]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splitDataSet(myDat, 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chooseBestFeatureToSplit(myDat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'no surfacing': {0: 'no', 1: {'flippers': {0: 'no', 1: 'yes'}}}}"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myDat, labels = createDataSet()\n",
    "myTree = createTree(myDat, labels)\n",
    "myTree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
