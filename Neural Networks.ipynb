{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes it's impossible to model the dependent variable Y as a linear equation of the independent variable X. We could model the dependent variable Y to be a high order polynomial equation of the dependent variable X, thus converting the problem into the standard form of linear regressions.\n",
    "\n",
    "It can also be seing that calculating the weights or coefficients of all the terms in a polynomial function using gradiend descent has a time complexity of O(n^2), where n is the numer of features in the training data. Similarly, the algorithmic complexity of calculating the coefficients of all the terms in a third-order polynomial equation is O(n^3). It's apparent that the time complexity of gradient descent increases geomtrically with the number of features of the model. Thus, gradient descent on its own is not efficient enough yo model nonlinear regression models with a large number of features.\n",
    "\n",
    "ANNs, on the other hand, are very efficient at modeling nonlinear regression models of data with a high number of features. ANNs are modeled form the behavior of the central nervous system of organisms. The brain processes information and generates electric signals that are transported thorugh the network of neural fibers to the various organs of the organism. **The actual processing of sensoty signals, however, is performed by several complex combinations of these neurons**. Of course, each neuron is capable of preocessing an exremely small portion og the information preocessed by the brain.\n",
    "\n",
    "The dendrites are used to receive signals from other neuros and can be thought of as the input to the neuron. Similarly, the axon the the neuron is analougous to the output of the neuron. The neuron can thus be mathematically represented as a function that processes several inputs and produces a single output. The interconnecting space between two neurons is called a **synapse**.\n",
    "\n",
    "**Perceptron** can be used on its own and is effective enough to estimate supervised machine learning models such as **linear regression** and **logistic regression**. However, complex nonlinear data can be better modeled with several interconnected perceptrons.\n",
    "\n",
    "ANNs can be braoadly classified into **feed-forward neural networks** and **recurrent neural networks**. The difference between these two types of ANNs is that in feed-forward neural networks, the conections between nodes ot the ANN do not form a directed cycle as opposed yo recurrent neural networks where the node interconnections do form a directed cycle. Thus, in feed-forward neural networks, each node in a given layer of the ANN receives input only from the nodes in the immediate previous layer of the ANN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multilayer Perceptron\n",
    "\n",
    "This model represents a basic feed-forward neural network and is versatile enough to model regression and classificaiton problem in the domain of supervised learning. All the inputs flow through a feed-forward neural network in a single direction. This is direct consequence of the fact that **there is no feddback** from or to any layer in a feed-forward neural network.\n",
    "\n",
    "Usin a single layer of perceptrons would mea using only a single activation function, which is the equivalent to using **logistic regression** to model the training data. This would mean that the model cannot fit nonlinear data, which is the primary innovation of ANNs.\n",
    "\n",
    "We can use a multilayer peceptron to perform a **binary or multiclass classifications**. A multilayer perceptron ANN can be trained using the **backpropagation algorithm**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backpropagation algorithm\n",
    "\n",
    "The backpropagation algorithm is used to train a multilayer perceptron ANN from a given set of sample values. A large number or iterations must be performed until the overall error in the ANN converges to a small value.\n",
    "The algorithm comprises three distinct phases.\n",
    "1. A forward propagation phase.\n",
    "    * Initialize the weights synapses of the ANN to random values.\n",
    "2. A backpropagation phase\n",
    "    * Error computation\n",
    "    * Calculate the product of the errors with the synape weights or input activations. This step produces the gradient.\n",
    "3. A weight update phase\n",
    "    * The learning rate and learning momentum come into play.\n",
    "4. Repeat 2 and 3 for the rest of the samples in the training data.    \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://upload.wikimedia.org/wikipedia/commons/6/6f/Gjl-t%28x%29.svg\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "Image(url='https://upload.wikimedia.org/wikipedia/commons/6/6f/Gjl-t%28x%29.svg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1.0/(1.0 + np.exp(-x))\n",
    "\n",
    "def sigmoid_prime(x): # derivative\n",
    "    return sigmoid(x)*(1.0-sigmoid(x))\n",
    "\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "def tanh_prime(x): # derivative\n",
    "    return 1.0 - x**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "\n",
    "    def __init__(self, layers, activation='tanh'):\n",
    "        if activation == 'sigmoid':\n",
    "            self.activation = sigmoid\n",
    "            self.activation_prime = sigmoid_prime\n",
    "        elif activation == 'tanh':\n",
    "            self.activation = tanh\n",
    "            self.activation_prime = tanh_prime\n",
    "\n",
    "        # Set weights\n",
    "        self.weights = []\n",
    "        # layers = [2,2,1]\n",
    "        # range of weight values (-1,1)\n",
    "        # input and hidden layers - random((2+1, 2+1)) : 3 x 3\n",
    "        for i in range(1, len(layers) - 1):\n",
    "            r = 2*np.random.random((layers[i-1] + 1, layers[i] + 1)) -1\n",
    "            self.weights.append(r)\n",
    "        # output layer - random((2+1, 1)) : 3 x 1\n",
    "        r = 2*np.random.random( (layers[i] + 1, layers[i+1])) - 1\n",
    "        self.weights.append(r)\n",
    "\n",
    "    def fit(self, X, y, learning_rate=0.2, epochs=100000):\n",
    "        # Add column of ones to X\n",
    "        # This is to add the bias unit to the input layer\n",
    "        ones = np.atleast_2d(np.ones(X.shape[0]))\n",
    "        X = np.concatenate((ones.T, X), axis=1)\n",
    "         \n",
    "        for k in range(epochs):\n",
    "            i = np.random.randint(X.shape[0])\n",
    "            a = [X[i]]\n",
    "\n",
    "            for l in range(len(self.weights)):\n",
    "                    dot_value = np.dot(a[l], self.weights[l])\n",
    "                    activation = self.activation(dot_value)\n",
    "                    a.append(activation)\n",
    "            # output layer\n",
    "            error = y[i] - a[-1]\n",
    "            deltas = [error * self.activation_prime(a[-1])]\n",
    "\n",
    "            # we need to begin at the second to last layer \n",
    "            # (a layer before the output layer)\n",
    "            for l in range(len(a) - 2, 0, -1): \n",
    "                deltas.append(deltas[-1].dot(self.weights[l].T)*self.activation_prime(a[l]))\n",
    "\n",
    "            # reverse\n",
    "            # [level3(output)->level2(hidden)]  => [level2(hidden)->level3(output)]\n",
    "            deltas.reverse()\n",
    "\n",
    "            # backpropagation\n",
    "            # 1. Multiply its output delta and input activation \n",
    "            #    to get the gradient of the weight.\n",
    "            # 2. Subtract a ratio (percentage) of the gradient from the weight.\n",
    "            for i in range(len(self.weights)):\n",
    "                layer = np.atleast_2d(a[i])\n",
    "                delta = np.atleast_2d(deltas[i])\n",
    "                self.weights[i] += learning_rate * layer.T.dot(delta)\n",
    "\n",
    "            if k % 10000 == 0: print 'epochs:', k\n",
    "\n",
    "    def predict(self, x): \n",
    "        a = np.concatenate((np.ones(1).T, np.array(x)), axis=0)      \n",
    "        for l in range(0, len(self.weights)):\n",
    "            a = self.activation(np.dot(a, self.weights[l]))\n",
    "        return a\n",
    "        \n",
    "    if __name__ == '__main__':\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epochs: 0\n",
      "epochs: 10000\n",
      "epochs: 20000\n",
      "epochs: 30000\n",
      "epochs: 40000\n",
      "epochs: 50000\n",
      "epochs: 60000\n",
      "epochs: 70000\n",
      "epochs: 80000\n",
      "epochs: 90000\n",
      "(array([0, 0]), array([  2.90456431e-06]))\n",
      "(array([0, 1]), array([ 0.99618694]))\n",
      "(array([1, 0]), array([ 0.99719177]))\n",
      "(array([1, 1]), array([ -1.84135890e-05]))\n"
     ]
    }
   ],
   "source": [
    "nn = NeuralNetwork([2,2,1])\n",
    "X = np.array([[0, 0],\n",
    "              [0, 1],\n",
    "              [1, 0],\n",
    "              [1, 1]])\n",
    "y = np.array([0, 1, 1, 0])\n",
    "nn.fit(X, y)\n",
    "for e in X:\n",
    "    print(e,nn.predict(e))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tiny Layer network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output After Training:\n",
      "[[ 0.00966449]\n",
      " [ 0.00786506]\n",
      " [ 0.99358898]\n",
      " [ 0.99211957]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# sigmoid function\n",
    "def nonlin(x,deriv=False):\n",
    "    if(deriv==True):\n",
    "        return x*(1-x)\n",
    "    return 1/(1+np.exp(-x))\n",
    "    \n",
    "# input dataset\n",
    "X = np.array([  [0,0,1],\n",
    "                [0,1,1],\n",
    "                [1,0,1],\n",
    "                [1,1,1] ])\n",
    "    \n",
    "# output dataset            \n",
    "y = np.array([[0,0,1,1]]).T\n",
    "\n",
    "# seed random numbers to make calculation\n",
    "# deterministic (just a good practice)\n",
    "np.random.seed(1)\n",
    "\n",
    "# initialize weights randomly with mean 0\n",
    "syn0 = 2*np.random.random((3,1)) - 1\n",
    "\n",
    "for iter in xrange(10000):\n",
    "\n",
    "    # forward propagation\n",
    "    l0 = X\n",
    "    l1 = nonlin(np.dot(l0,syn0))\n",
    "\n",
    "    # how much did we miss?\n",
    "    l1_error = y - l1\n",
    "\n",
    "    # multiply how much we missed by the \n",
    "    # slope of the sigmoid at the values in l1\n",
    "    l1_delta = l1_error * nonlin(l1,True)\n",
    "\n",
    "    # update weights\n",
    "    syn0 += np.dot(l0.T,l1_delta)\n",
    "\n",
    "print \"Output After Training:\"\n",
    "print l1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
