{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mathematical Optimization\n",
    "\n",
    "Mathematical Optimization deals with the problem of finding numerically minimums (or maximums or zeros) of a function. In this context, the function is called **cost function**, **objective function** or **energy**.\n",
    "\n",
    "### Dimensionality of the problem\n",
    "\n",
    "The scale of an optimization problem is set by the dimensionality of the problem, i.e. the number of scalar variables on which the search is performed. **Optimizing smooth function is easier**\n",
    "\n",
    "Types of problems\n",
    "* Convex vs non-convex\n",
    "* Smooth vs non-smooth\n",
    "* Noisy vs non-noisy\n",
    "* Optimization under constraints or without them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Brent’s method \n",
    "**On a quadratic function ** it converges in 3 iterations, as the quadratic approximation is then exact. Brent’s method on a **non-convex function**,  the fact that the optimizer avoided the local minimum is a matter of luck."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from scipy import optimize\n",
    "\n",
    "def f_cuadratic(x):\n",
    "    return -np.exp(-(x - .7)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x10767f090>]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAEACAYAAABfxaZOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x107612250>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = np.arange(-5,10)\n",
    "plt.plot(x, f_cuadratic(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.699999999784\n",
      "-2.16059059532e-10\n"
     ]
    }
   ],
   "source": [
    "x_min = optimize.brent(f)\n",
    "print x_min\n",
    "print x_min - .7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient based methods\n",
    "\n",
    "At a theoretical level, gradient descent is an algorithm that minimizes functions. Given a function defined by a ser of parameters, gradient descent starts with an initial set of parameters values and iteratively moves toward a set of parameters values that minimize the function.Gradient descent basically consists in taking small steps in the direction of the gradient, that is the direction of the steepest descent; taking steps in the negative direction of the function gradient. The core problem of gradient-methods on ill-conditioned problems is that the gradient tends no to point in the direction of the minimum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"800\"\n",
       "            height=\"400\"\n",
       "            src=\"https://en.wikipedia.org/wiki/Preconditioner\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x107612450>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import IFrame\n",
    "IFrame('https://en.wikipedia.org/wiki/Preconditioner', width=800, height=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conjugate gradient descent\n",
    "\n",
    "Some gradient descent algorithms are toys not to be used on real problems. One of the problems of the simple gradient descent algorithms, is that they tend to oscilate across a valley, each time following the direciton of the fradient, that makes it cross the valley. The conjugate gradient solves this problem by adding a friction term: each step depends on the two last values on the gradiend and shart turns to be reduced. To run gradient descent on an error function, we first need to compute its gradient. The gradient will act like a compass and always point us downhill\n",
    "\n",
    "These methods need the gradient of the function. They can compute it, but will perform better if you can pass them the gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    return .5*(1 - x[0])**2 + (x[1] - x[0]**2) ** 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.000000\n",
      "         Iterations: 13\n",
      "         Function evaluations: 120\n",
      "         Gradient evaluations: 30\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0.99998968,  0.99997855])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimize.fmin_cg(f, [2,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fprime(x): # The rosenbrock function\n",
    "    return np.array((-2*.5*(1 - x[0]) - 4*x[0]*(x[1] - x[0]**2), 2*(x[1] - x[0]**2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.000000\n",
      "         Iterations: 13\n",
      "         Function evaluations: 30\n",
      "         Gradient evaluations: 30\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0.99999199,  0.99998336])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Only 30 evaluations, compared with 120 without gradient\n",
    "optimize.fmin_cg(f, [2,2], fprime=fprime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_error(b,m, points):\n",
    "    total_error = 0\n",
    "    for i in range(0, len(points)):\n",
    "        totalError += (points[i].y - (m * points[i].x + b)**2)\n",
    "    return total_error / float(len(points))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
